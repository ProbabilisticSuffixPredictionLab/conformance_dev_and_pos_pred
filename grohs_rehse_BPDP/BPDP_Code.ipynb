{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# We import the neccessary packages in the beginning\n",
    "import os\n",
    "import math\n",
    "from statistics import mean,stdev\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.conversion.bpmn import converter as bpmn_converter\n",
    "from sklearn.impute import SimpleImputer\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sklearn\n",
    "import tqdm\n",
    "import time\n",
    "import xgboost as xgb"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Returns a path to the file selected by the user\n",
    "# Input: The folder in which to look for the files - the default is the current folder\n",
    "def ask_for_path(rel_path='', index = -1):\n",
    "    #Crawl all files in the input folder\n",
    "    print(\"The following files are available in the input folder:\\n\")\n",
    "\n",
    "    count = 0\n",
    "    file_list = os.listdir(os.getcwd() + rel_path)\n",
    "    for file in file_list:\n",
    "        print(str(count) + \" - \" + file)\n",
    "        count+=1\n",
    "\n",
    "    if(index == -1):\n",
    "        #Ask for which of the files shall be transformed and select it.\n",
    "        inp = input(\"Please choose from the list above which of the files shall be transformed by typing the corresponding number.\")\n",
    "    else:\n",
    "        #Automatic iteration\n",
    "        print('Automatic Iteration.')\n",
    "        inp = index\n",
    "\n",
    "    input_file = file_list[int(inp)]\n",
    "\n",
    "    return (os.getcwd() + rel_path + input_file)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a help function to print petri nets\n",
    "def output_petri_net(net, initial_marking, final_marking, file_name, label):\n",
    "\n",
    "    #init visualizer\n",
    "    parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: OUTPUT_FORMAT, 'label':'The Round Table'}   #Add frequency to graph\n",
    "    gviz = pn_visualizer.apply(net, initial_marking, final_marking, parameters=parameters,\n",
    "                               variant=pn_visualizer.Variants.FREQUENCY, log=log)\n",
    "\n",
    "    gviz.attr(label=label)\n",
    "    pn_visualizer.save(gviz, os.getcwd() + REL_OUTPUT_PATH + file_name + \".\" + OUTPUT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(file_name,REL_OUTPUT_PATH = \"/Output Tree/\"):\n",
    "    return (os.getcwd() + REL_OUTPUT_PATH + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts a selected file in the path that is the input into a log\n",
    "def transform_to_log(file_path):\n",
    "    filename, file_extension = os.path.splitext(file_path)\n",
    "    x,z =os.path.split(file_path)\n",
    "    \n",
    "    if file_extension == '.csv':\n",
    "        log_csv = pd.read_csv(file,sep=None,encoding='utf-8-sig')\n",
    "        if z =='mobis_challenge_log_2019.csv' or z =='mobis_challenge_log_2019_only_complete_cases.csv':\n",
    "            log_csv['end'] = pd.to_datetime(log_csv['end'])\n",
    "            log_csv['start'] = pd.to_datetime(log_csv['start'])\n",
    "            log_csv['cost'] = log_csv['cost'].apply(pd.to_numeric, errors='coerce')\n",
    "            log_csv.rename(columns={'cost': 'case:cost','case':'case:concept:name','activity':'concept:name','end':'time:timestamp', 'user':'org:resource'}, inplace=True)\n",
    "        elif z =='mobis_challenge_log_2019_original.csv':\n",
    "            log_csv['end'] = pd.to_datetime(log_csv['end'])\n",
    "            log_csv['start'] = pd.to_datetime(log_csv['start'])\n",
    "            log_csv['cost'] = log_csv['cost'].apply(pd.to_numeric, errors='coerce')\n",
    "            log_csv.rename(columns={'case':'case:concept:name','activity':'concept:name','start':'time:timestamp', 'user':'org:resource'}, inplace=True)\n",
    "        log_csv['time:timestamp'] = pd.to_datetime(log_csv['time:timestamp'], format='mixed')\n",
    "        log = log_converter.apply(log_csv)\n",
    "\n",
    "    elif file_extension == '.xes':\n",
    "        log = pm4py.read_xes(file_path)\n",
    "        log = pm4py.convert_to_event_log(log)\n",
    "    elif file_extension == '.dfg':\n",
    "        log = pm4py.read_dfg(file_path)\n",
    "    else:\n",
    "        print(\"Current filetype is equal to {}. \\nPlease input a file with any of the following extensions: - csv; - xes; - dfg\".format(str(file_extension)))\n",
    "        return -1\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_activities_from_log(log):\n",
    "    activities=[]\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            if activities.count(event['concept:name'])==0:\n",
    "                activities.append(event['concept:name'])\n",
    "    return activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function enriches each trace by the event 1...m, resource 1...m, Weekday start and end attributes until a given prefix length\n",
    "def complex_index_encoding(log, pref_length=5):\n",
    "    max_ev=0\n",
    "    for trace in log:\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "    \n",
    "    if pref_length > max_ev:\n",
    "        print('The prefix length is larger than the maximum trace length; Maximum trace length will be used.')\n",
    "        pref_length = max_ev\n",
    "\n",
    "    #weekdays\n",
    "    weekDaysMapping = (\"Monday\", \"Tuesday\",\n",
    "                    \"Wednesday\", \"Thursday\",\n",
    "                    \"Friday\", \"Saturday\",\n",
    "                    \"Sunday\")\n",
    "\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            trace.attributes['weekday_start']=weekDaysMapping[event['time:timestamp'].weekday()]\n",
    "            break\n",
    "    if pref_length == max_ev:\n",
    "        for trace in log:\n",
    "            for event in trace:\n",
    "                trace.attributes['weekday_end']=weekDaysMapping[event['time:timestamp'].weekday()]\n",
    "    \n",
    "    \n",
    "    j=0\n",
    "    no_evs={}\n",
    "    for trace in log:\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            if i==1:\n",
    "                st_time=event['time:timestamp'].day+event['time:timestamp'].hour/24+event['time:timestamp'].minute/(24*60)\n",
    "            if i<= pref_length:\n",
    "                trace.attributes['event_'+str(i)]=event['concept:name']\n",
    "                trace.attributes['resource_'+str(i)]=str(event['org:resource'])\n",
    "                trace.attributes['month_'+str(i)]=(str(event['time:timestamp'].month)+'_'+str(event['time:timestamp'].year))\n",
    "                #trace.attributes['elapsed_time']=event['time:timestamp'].day+event['time:timestamp'].hour/24+event['time:timestamp'].minute/(24*60)-st_time\n",
    "        no_evs[j]=i\n",
    "        j+=1\n",
    "\n",
    "    j=0\n",
    "    for trace in log:\n",
    "        if no_evs[j]<max_ev:\n",
    "            fill=no_evs[j]+1\n",
    "            for k in range(fill,max(max_ev,pref_length)+1):\n",
    "                trace.attributes['event_'+str(k)]=np.nan\n",
    "                trace.attributes['resource_'+str(k)]=np.nan\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffeine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "\"\"\"Settings\"\"\"\n",
    "##########\n",
    "# set the input and output path according to the files you want to select\n",
    "REL_INPUT_PATH = \"/../BPIC12/\" # here lie the event logs (.csv), the to-be model (.bpmn) and the already aligned traces (.pkl)\n",
    "REL_OUTPUT_PATH = \"/../BPIC12/\"\n",
    "OUTPUT_FORMAT = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the log from the input path\n",
    "file= ask_for_path(REL_INPUT_PATH,11) # adjust to your path\n",
    "log=transform_to_log(file)\n",
    "ref_log=transform_to_log(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= ask_for_path(REL_INPUT_PATH,1)# adjust to your path\n",
    "bpmn_graph = pm4py.read_bpmn(file)\n",
    "#pm4py.write_bpmn(bpmn_graph, \"ru.bpmn\", enable_layout=True)\n",
    "net, initial_marking, final_marking = bpmn_converter.apply(bpmn_graph)\n",
    "#net, initial_marking, final_marking=pm4py.read_pnml(file)\n",
    "# pm4py.visualization.petri_net.visualizer(net, initial_marking, final_marking)\n",
    "# output_petri_net(net, initial_marking, final_marking,'Basis_PN', 'test')\n",
    "pm4py.view_petri_net(net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_pkl(log, net, initial_marking, final_marking):\n",
    "    aligned_traces = pm4py.conformance_diagnostics_alignments(log, net, initial_marking, final_marking)\n",
    "    i=0\n",
    "    dev=[]\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i+=1\n",
    "\n",
    "    f = open('aligned_traces_binet_12A.pkl','wb')\n",
    "    pickle.dump(aligned_traces,f)\n",
    "    f.close()\n",
    "    return dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dev, aligned_traces=generate_alignments_pkl(log, net, initial_marking, final_marking)\n",
    "#print(len(dev))\n",
    "#dev"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= ask_for_path(REL_INPUT_PATH,15)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    aligned_traces=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "## test data    \n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define our FFN \n",
    "class BinaryClassificationIndiv(nn.Module):\n",
    "    def __init__(self, no_columns):\n",
    "        super(BinaryClassificationIndiv, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(no_columns, 256)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(256, 256)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(256, 2)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(256)\n",
    "        self.batchnorm2 = nn.LayerNorm(256)\n",
    "        self.Softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we define our FFN\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self, no_columns, no_devs):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(no_columns, 2048)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(2048, 2048)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_3 = nn.Linear(2048, 1024)\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(1024, no_devs)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(2048)\n",
    "        self.batchnorm2 = nn.LayerNorm(1024)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation3(self.layer_3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BPDP_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_events, vocab_resources, no_TA, vocab_month):\n",
    "        super(BPDP_LSTM, self).__init__()\n",
    "        self.embedding_e = nn.Embedding(vocab_events, 16) # hier auf 8 / 16\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.lstm_e = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "        self.linear_e = nn.Linear(64, 32)\n",
    "        self.embedding_r = nn.Embedding(vocab_resources, 16)\n",
    "        self.lstm_r = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "        self.linear_r = nn.Linear(64, 32)\n",
    "        self.embedding_m = nn.Embedding(vocab_month, 16)\n",
    "        self.lstm_m = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "        self.linear_m = nn.Linear(64, 32)\n",
    "        self.linear_ta = nn.Linear(no_TA, 32)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(128)\n",
    "        self.linear = nn.Linear(128, 2)\n",
    "    def forward(self, evs, rs,tas, ms):\n",
    "        evs= self.embedding_e(evs)\n",
    "        evs, _ = self.lstm_e(evs)\n",
    "        evs=self.linear_e(evs)\n",
    "        evs=evs[:, -1, :]\n",
    "        evs=self.activation1(evs)\n",
    "        rs= self.embedding_r(rs)\n",
    "        rs, _ = self.lstm_r(rs)\n",
    "        rs=rs[:, -1, :]\n",
    "        rs=self.activation1(rs)\n",
    "        rs=self.linear_r(rs)\n",
    "        ms= self.embedding_m(ms)\n",
    "        ms, _ = self.lstm_m(ms)\n",
    "        ms=ms[:, -1, :]\n",
    "        ms=self.activation1(ms)\n",
    "        ms=self.linear_m(ms)\n",
    "        tas= self.linear_ta(tas)\n",
    "        fin=torch.cat((evs,rs),dim=1)\n",
    "        fin=torch.cat((fin,ms),dim=1)\n",
    "        fin=torch.cat((fin,tas),dim=1)\n",
    "        fin=self.batchnorm1(fin)\n",
    "        #fin = self.dropout(fin)\n",
    "        fin = self.linear(fin)\n",
    "        return fin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LargerBinaryClassificationIndiv(nn.Module):\n",
    "    def __init__(self, no_columns):\n",
    "        super(LargerBinaryClassificationIndiv, self).__init__()\n",
    "        self.layer_1 = nn.Linear(no_columns, 512)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(512, 256)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_3 = nn.Linear(256, 256)\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(256, 2)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(512)\n",
    "        self.batchnorm2 = nn.LayerNorm(256)\n",
    "        self.Softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation3(self.layer_3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "  def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss <= self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for printing the accuracy during training - only informational\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.nn.functional.softmax(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,explained=False):\n",
    "    xt,z =os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i=0\n",
    "    dev=[] # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i+=1\n",
    "\n",
    "    y_cum_test={} # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df=pd.DataFrame(data=0,columns=dev, index=range(len(log))) # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order={} # dict with event sequences for each trace\n",
    "    event_count={} # dict with trace length for each trace\n",
    "    max_ev=0 # will be maximum trace length\n",
    "    k=0\n",
    "    for trace in log:\n",
    "        event_order[k]=[]\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "        event_count[k]=len(event_order[k])\n",
    "        k+=1\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i]=1\n",
    "        i+=1\n",
    "    for ev in range(1,max_ev+1):\n",
    "        y_cum_test[ev]=dev_df.copy() # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< ev:\n",
    "                drop_idx.append(trace_idx) # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev]=y_cum_test[ev].drop(drop_idx)\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        j=no_moves-1 # iterator over moves in alignment, starting at the end\n",
    "        m=len(event_order[i]) # iterator over event sequence, starting at the end\n",
    "        while j >=0:\n",
    "            if aligned_traces[i]['alignment'][j][1] == None: # if silent move, just go one move further to the beginning in the alignment\n",
    "                j-=1\n",
    "            elif aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j-=1\n",
    "                    m-=1\n",
    "            elif event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # log move detected\n",
    "                for q in range(m,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j-=1\n",
    "                m-=1\n",
    "            elif m==max_ev:\n",
    "                j-=1\n",
    "            else: # model move deteceted\n",
    "                for q in range(m+1,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j-=1\n",
    "        i+=1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000) # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe=ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe=ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe=ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe=ref_dataframe.reset_index()\n",
    "    ref_raw_dat=ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ']= pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat=ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z=='aligned_traces_20int.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20dom.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "    elif z=='aligned_traces_20prep.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20RfP.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat=ref_raw_dat.copy()\n",
    "    ref_enc_dat=pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE=0.0001\n",
    "\n",
    "    X_cum={}\n",
    "    metrics=pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC','LenTrain', 'LenTrain_beforeUS_0', 'LenTrain_beforeUS_1', 'LenTrain_afterUS_0', 'LenTrain_afterUS_1'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        complex_index_encoding(log,prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe=dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe=dataframe.filter(like='case:', axis=1)\n",
    "        dataframe=dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe=dataframe.reset_index()\n",
    "        raw_dat=dataframe.drop('index', axis=1)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ']= pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat=raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z=='aligned_traces_20int.pkl':\n",
    "            clean_dat=raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20dom.pkl':\n",
    "            clean_dat=raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "        elif z=='aligned_traces_20prep.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20RfP.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat=raw_dat.copy()\n",
    "        enc_dat=pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key]=0 # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "        enc_dat=pd.DataFrame(data=imp.fit_transform(enc_dat),columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix]=enc_dat.copy()\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix]=X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "\n",
    "\n",
    "\n",
    "    path=(os.getcwd()+'/BPDP_Classifier') # output path\n",
    "    xt,z =os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path+'/'+z+'_BPDP_CIBE_classification_sd_early.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev'+d)]=0\n",
    "        metrics[d]['LenTrain_beforeUS_1']=sum(dev_df[d][i] for i in x_train_idx)\n",
    "        metrics[d]['LenTrain_beforeUS_0']=len(x_train_idx)-sum(dev_df[d][i] for i in x_train_idx)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx]+1):\n",
    "                if y_cum_test[i][d][idx]==1: dev_position[d][idx]=i+1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness={}\n",
    "    sd_earliness={}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training','Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training']=sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test']=sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    dev_trained=[]\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] ==0:\n",
    "            metrics[d]='No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] ==0:\n",
    "            metrics[d]='No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "\n",
    "\n",
    "        Y_cum_dev={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            Y_cum_dev[prefix]=pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev']=0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i]=1-y_cum_test[prefix][d][i]\n",
    "            if prefix==1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat=ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat['ind']=0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i]=i\n",
    "            imb_traces=pd.DataFrame(data=0, columns=['Dev'], index = range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace]>0:\n",
    "                    imb_traces['Dev'][trace]=1\n",
    "\n",
    "\n",
    "            imb_traces=imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat=imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "            imb_ref_enc_dat=pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat),columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss=OneSidedSelection(random_state=0,n_seeds_S=250,n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx= list(X_resampled['ind'])\n",
    "            y_train_idx= list(X_resampled['ind'])\n",
    "        else:\n",
    "            x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "        print('index length ', len(x_train_idx),len(x_test_idx),len(y_train_idx),len(y_test_idx))\n",
    "        metrics[d]['LenTrain']=len(x_train_idx)\n",
    "        metrics[d]['LenTrain_afterUS_1']=imb_traces['Dev'].sum()\n",
    "        metrics[d]['LenTrain_afterUS_0']=len(x_train_idx)-imb_traces['Dev'].sum()\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "        enumerated_trace_idx={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            drop_idx=[]\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx]< prefix:\n",
    "                    drop_idx.append(trace_idx) # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te=X_cum[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr=X_cum[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va=X_cum[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te=Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr=Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va=Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix]=list(set(y_test_idx)-set(drop_idx))\n",
    "            print('subset length ',prefix, len(x_te),len(x_tr),len(y_te),len(y_tr))\n",
    "\n",
    "            if prefix ==1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append( X_train, x_tr, axis=0)\n",
    "                X_test = np.append( X_test, x_te, axis=0)\n",
    "                y_train = np.append( y_train, y_tr, axis=0)\n",
    "                y_test = np.append( y_test, y_te, axis=0)\n",
    "                y_val = np.append( y_val, y_va, axis=0)\n",
    "                X_val = np.append( X_val, x_va, axis=0)# combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train),len(y_train),len(X_val),len(y_val),len(X_test),len(y_test))\n",
    "\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS=300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch<EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss=0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device))/len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "                    epoch_loss+=loss\n",
    "                    if i == len(steps)-1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model,vloss): done = True\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss: {epoch_loss/len(train_loader)}, Acc: {epoch_acc/len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss {epoch_loss/len(train_loader):}, Acc: {epoch_acc/len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision']=CM[0][1][1]/(CM[0][1][1]+CM[0][0][1])\n",
    "        metrics[d]['Recall']=CM[0][1][1]/(CM[0][1][1]+CM[0][1][0])\n",
    "        metrics[d]['Support']=CM[0][1][1]+CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] =  sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev'+d)]['Precision']=CM[1][1][1]/(CM[1][1][1]+CM[1][0][1])\n",
    "        metrics[str('NoDev'+d)]['Recall']=CM[1][1][1]/(CM[1][1][1]+CM[1][1][0])\n",
    "        metrics[str('NoDev'+d)]['Support']=CM[1][1][1]+CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        to_be_checked_idx={}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx=0\n",
    "            for prefix in range(1,event_count[idx]+1):\n",
    "                if prefix==1:\n",
    "                    to_be_checked_idx[idx]=[enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx)+cum_idx)\n",
    "                cum_idx+=len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "        for prefix in range(1, max_ev+1):\n",
    "            for idx in to_be_checked_idx.keys():\n",
    "                if event_count[idx]>= prefix:\n",
    "                    if prefix==1:\n",
    "                        dev_position_pred[d][idx]=y_pred_list[to_be_checked_idx[idx][prefix-1]][0]\n",
    "                    else:\n",
    "                        if y_pred_list[to_be_checked_idx[idx][prefix-1]][0]==1 and y_pred_list[to_be_checked_idx[idx][prefix-2]][0]==0:\n",
    "                            if dev_position[d][idx]<= prefix:\n",
    "                                dev_position_pred[d][idx]=dev_position[d][idx]\n",
    "                            else:\n",
    "                                dev_position_pred[d][idx]=prefix\n",
    "\n",
    "\n",
    "\n",
    "        earliness[d]=0\n",
    "        tobe_devs=0\n",
    "        sd_earliness[d]=0\n",
    "        earliness_list=[]\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx]==0 or dev_position_pred[d][idx]==0:\n",
    "                continue\n",
    "            tobe_devs+=1\n",
    "            earliness[d]+=dev_position_pred[d][idx]/dev_position[d][idx]\n",
    "            earliness_list.append(dev_position_pred[d][idx]/dev_position[d][idx])\n",
    "        if not tobe_devs==0:\n",
    "            earliness[d]=earliness[d]/tobe_devs\n",
    "            try:\n",
    "                sd_earliness[d]=stdev(earliness_list)\n",
    "            except:\n",
    "                sd_earliness[d]=earliness_list[0]\n",
    "                print(earliness_list)\n",
    "\n",
    "        if explained:\n",
    "            import shap\n",
    "            import matplotlib.pyplot as plt\n",
    "            np.random.seed(42)\n",
    "            e = shap.DeepExplainer(model, torch.FloatTensor(X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]))\n",
    "\n",
    "            shap_idx=[]\n",
    "            for j in range(len(y_pred_list)):\n",
    "                if y_pred_list[j][0]==y_test[j][0]==1:\n",
    "                    shap_idx.append(j)\n",
    "            shap_values = e.shap_values(torch.FloatTensor(X_test[shap_idx]))\n",
    "            fig=shap.summary_plot(shap_values[0], X_test[shap_idx], plot_type = 'dot', feature_names = enc_dat.columns, max_display=10, plot_size=(10,5), show=False)\n",
    "            plt.savefig(path+'/ShapValues/Dev_'+z+'_'+d+'.png')\n",
    "            plt.close()\n",
    "\n",
    "            fig=shap.summary_plot(shap_values[1], X_test[shap_idx], plot_type = 'dot', feature_names = enc_dat.columns, max_display=10, plot_size=(10,5), show=False)\n",
    "            plt.savefig(path+'/ShapValues/NoDev_'+z+'_'+d+'.png')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    avg_dev_pos={}\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Test'] ==0:\n",
    "            metrics[d]='No Deviation in Test Set'\n",
    "            continue\n",
    "        if dev_distribution[d]['Training'] ==0:\n",
    "            metrics[d]='No Deviation in Training Set'\n",
    "            continue\n",
    "        devs=0\n",
    "        positions=0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx]>0:\n",
    "                devs+=1\n",
    "                positions+=dev_position[d][idx]\n",
    "        if devs ==0:\n",
    "            continue\n",
    "        avg_dev_pos[d]=positions/devs\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    df=pd.DataFrame(data=earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness'))\n",
    "    df=pd.DataFrame(data=sd_earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness_SD'))\n",
    "    df=pd.DataFrame(data=avg_dev_pos, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Position'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,explained=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_LSTM_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "\n",
    "    ref_enc_dat = ref_clean_dat.copy()\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.00001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC','Time'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = clean_dat.copy()\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 'No'  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='No')\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_LSTM')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter('BPDP_LSTM' + '/' + z + '_BPDP_LSTM_time_stopped.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    ref_enc_dat\n",
    "\n",
    "    evs_c = []\n",
    "    resource_c = []\n",
    "    month_c = []\n",
    "    trace_attr = []\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('event'): evs_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('resource'): resource_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('month'): month_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if not (ca in evs_c or ca in resource_c or ca in month_c): trace_attr.append(ca)\n",
    "    print(evs_c)\n",
    "    print(resource_c)\n",
    "    print(month_c)\n",
    "    print(trace_attr)\n",
    "\n",
    "    X_events = {}\n",
    "    X_resource = {}\n",
    "    X_month = {}\n",
    "    X_tracea = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        X_events[prefix] = X_cum[prefix][evs_c]\n",
    "        X_resource[prefix] = X_cum[prefix][resource_c]\n",
    "        X_month[prefix] = X_cum[prefix][month_c]\n",
    "        X_tracea[prefix] = X_cum[prefix][trace_attr]\n",
    "\n",
    "    cat_tas = []\n",
    "    for cat in X_tracea[1].columns:\n",
    "        if type(X_tracea[1][cat][0]) == str:\n",
    "            cat_tas.append(cat)\n",
    "\n",
    "    uniques_cats = {}\n",
    "    for cat in cat_tas:\n",
    "        uniques_cats[cat] = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        for cat in cat_tas:\n",
    "            for reals in list(X_tracea[prefix][cat].unique()):\n",
    "                if not reals in uniques_cats[cat]:\n",
    "                    uniques_cats[cat].append(reals)\n",
    "\n",
    "    for cat in cat_tas:\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for j in list(X_tracea[prefix].index):\n",
    "                X_tracea[prefix][cat][j] = uniques_cats[cat].index(X_tracea[prefix][cat][j])\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "    models_collect = {}\n",
    "    dev_trained = []\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "    for d in dev:\n",
    "        time_start = time.clock()\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat = pd.get_dummies(imb_ref_enc_dat)\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_events[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_events[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_events[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_te_c = X_events[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_events[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_events[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "                [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "            pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_event = x_tr\n",
    "                X_test_event = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val_event = x_va\n",
    "                y_val = y_va\n",
    "                X_train_event_c = x_tr_c\n",
    "                X_test_event_c = x_te_c\n",
    "                X_val_event_c = x_va_c\n",
    "                y_train_c = y_tr_c\n",
    "                y_test_c = y_te_c\n",
    "                y_val_c = y_va_c\n",
    "            else:\n",
    "                X_train_event = np.append(X_train_event, x_tr, axis=0)\n",
    "                X_test_event = np.append(X_test_event, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val_event = np.append(X_val_event, x_va, axis=0)\n",
    "                X_train_event_c = np.append(X_train_event_c, x_tr_c, axis=0)\n",
    "                X_test_event_c = np.append(X_test_event_c, x_te_c, axis=0)\n",
    "                X_val_event_c = np.append(X_val_event_c, x_va_c, axis=0)\n",
    "                y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "                y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "                y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_event), len(y_train), len(y_train_c), len(X_val_event), len(y_val), len(y_val_c),\n",
    "              len(X_test_event), len(y_test), len(y_test_c))\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_resource[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_resource[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_resource[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_resource[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_resource[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_resource[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_resource = x_tr\n",
    "                X_test_resource = x_te\n",
    "                X_val_resource = x_va\n",
    "                X_train_resource_c = x_tr_c\n",
    "                X_test_resource_c = x_te_c\n",
    "                X_val_resource_c = x_va_c\n",
    "            else:\n",
    "                X_train_resource = np.append(X_train_resource, x_tr, axis=0)\n",
    "                X_test_resource = np.append(X_test_resource, x_te, axis=0)\n",
    "                X_val_resource = np.append(X_val_resource, x_va, axis=0)\n",
    "                X_train_resource_c = np.append(X_train_resource_c, x_tr_c, axis=0)\n",
    "                X_test_resource_c = np.append(X_test_resource_c, x_te_c, axis=0)\n",
    "                X_val_resource_c = np.append(X_val_resource_c, x_va_c,\n",
    "                                             axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_resource), len(y_train), len(X_val_resource), len(y_val), len(X_test_resource), len(y_test))\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_month[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_month[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_month[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_month[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_month[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_month[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_m = x_tr\n",
    "                X_test_m = x_te\n",
    "                X_val_m = x_va\n",
    "                X_train_m_c = x_tr_c\n",
    "                X_test_m_c = x_te_c\n",
    "                X_val_m_c = x_va_c\n",
    "            else:\n",
    "                X_train_m = np.append(X_train_m, x_tr, axis=0)\n",
    "                X_test_m = np.append(X_test_m, x_te, axis=0)\n",
    "                X_val_m = np.append(X_val_m, x_va, axis=0)\n",
    "                X_train_m_c = np.append(X_train_m_c, x_tr_c, axis=0)\n",
    "                X_test_m_c = np.append(X_test_m_c, x_te_c, axis=0)\n",
    "                X_val_m_c = np.append(X_val_m_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_m), len(y_train), len(X_val_m), len(y_val), len(X_test_m), len(y_test))\n",
    "\n",
    "        print('split done')\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_tracea[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_tracea[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_tracea[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_tracea[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_tracea[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_tracea[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_TA = x_tr\n",
    "                X_test_TA = x_te\n",
    "                X_val_TA = x_va\n",
    "                X_train_TA_c = x_tr_c\n",
    "                X_test_TA_c = x_te_c\n",
    "                X_val_TA_c = x_va_c\n",
    "            else:\n",
    "                X_train_TA = np.append(X_train_TA, x_tr, axis=0)\n",
    "                X_test_TA = np.append(X_test_TA, x_te, axis=0)\n",
    "                X_val_TA = np.append(X_val_TA, x_va, axis=0)\n",
    "                X_train_TA_c = np.append(X_train_TA_c, x_tr_c, axis=0)\n",
    "                X_test_TA_c = np.append(X_test_TA_c, x_te_c, axis=0)\n",
    "                X_val_TA_c = np.append(X_val_TA_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_TA), len(y_train), len(X_val_TA), len(y_val), len(X_test_TA), len(y_test))\n",
    "\n",
    "        events_encoder = list(\n",
    "            np.unique(np.append(np.append(X_train_event_c, X_test_event_c, axis=0), X_val_event_c, axis=0)))\n",
    "        events_encoder.index('No')\n",
    "        resource_encoder = list(\n",
    "            np.unique(np.append(np.append(X_train_resource_c, X_test_resource_c, axis=0), X_val_resource_c, axis=0)))\n",
    "        resource_encoder.index('No')\n",
    "        cat_ecnoders = {}\n",
    "        for cat in cat_tas:\n",
    "            cat_ecnoders[cat] = list(np.unique(np.append(np.append(X_train_TA_c, X_test_TA_c, axis=0), X_val_TA_c, axis=0)))\n",
    "\n",
    "        month_encoder = list(np.unique(np.append(np.append(X_train_m_c, X_test_m_c, axis=0), X_val_m_c, axis=0)))\n",
    "        month_encoder\n",
    "        for i in range(len(X_test_event)):\n",
    "            for j in range(len(X_test_event[0])):\n",
    "                X_test_event[i][j] = events_encoder.index(X_test_event[i][j])\n",
    "            for j in range(len(X_test_resource[0])):\n",
    "                X_test_resource[i][j] = resource_encoder.index(X_test_resource[i][j])\n",
    "            for j in range(len(X_test_m[0])):\n",
    "                X_test_m[i][j] = month_encoder.index(X_test_m[i][j])\n",
    "        for i in range(len(X_train_event)):\n",
    "            for j in range(len(X_train_event[0])):\n",
    "                X_train_event[i][j] = events_encoder.index(X_train_event[i][j])\n",
    "            for j in range(len(X_train_resource[0])):\n",
    "                X_train_resource[i][j] = resource_encoder.index(X_train_resource[i][j])\n",
    "            for j in range(len(X_train_m[0])):\n",
    "                X_train_m[i][j] = month_encoder.index(X_train_m[i][j])\n",
    "        for i in range(len(X_val_event)):\n",
    "            for j in range(len(X_val_event[0])):\n",
    "                X_val_event[i][j] = events_encoder.index(X_val_event[i][j])\n",
    "            for j in range(len(X_val_resource[0])):\n",
    "                X_val_resource[i][j] = resource_encoder.index(X_val_resource[i][j])\n",
    "            for j in range(len(X_val_m[0])):\n",
    "                X_val_m[i][j] = month_encoder.index(X_val_m[i][j])\n",
    "\n",
    "        # for combs_output\n",
    "        for i in range(len(X_test_event_c)):\n",
    "            for j in range(len(X_test_event_c[0])):\n",
    "                X_test_event_c[i][j] = events_encoder.index(X_test_event_c[i][j])\n",
    "            for j in range(len(X_test_resource[0])):\n",
    "                X_test_resource_c[i][j] = resource_encoder.index(X_test_resource_c[i][j])\n",
    "            for j in range(len(X_test_m[0])):\n",
    "                X_test_m_c[i][j] = month_encoder.index(X_test_m_c[i][j])\n",
    "        for i in range(len(X_train_event_c)):\n",
    "            for j in range(len(X_train_event_c[0])):\n",
    "                X_train_event_c[i][j] = events_encoder.index(X_train_event_c[i][j])\n",
    "            for j in range(len(X_train_resource[0])):\n",
    "                X_train_resource_c[i][j] = resource_encoder.index(X_train_resource_c[i][j])\n",
    "            for j in range(len(X_train_m[0])):\n",
    "                X_train_m_c[i][j] = month_encoder.index(X_train_m_c[i][j])\n",
    "        for i in range(len(X_val_event_c)):\n",
    "            for j in range(len(X_val_event_c[0])):\n",
    "                X_val_event_c[i][j] = events_encoder.index(X_val_event_c[i][j])\n",
    "            for j in range(len(X_val_resource_c[0])):\n",
    "                X_val_resource_c[i][j] = resource_encoder.index(X_val_resource_c[i][j])\n",
    "            for j in range(len(X_val_m_c[0])):\n",
    "                X_val_m_c[i][j] = month_encoder.index(X_val_m_c[i][j])\n",
    "        scaler = StandardScaler()\n",
    "        X_test_TA = scaler.fit_transform(X_test_TA)\n",
    "        X_train_TA = scaler.fit_transform(X_train_TA)\n",
    "        X_val_TA = scaler.fit_transform(X_val_TA)\n",
    "        X_test_TA_c = scaler.fit_transform(X_test_TA_c)\n",
    "        X_train_TA_c = scaler.fit_transform(X_train_TA_c)\n",
    "        X_val_TA_c = scaler.fit_transform(X_val_TA_c)\n",
    "        X_test_event = X_test_event.astype(int)\n",
    "        X_train_event = X_train_event.astype(int)\n",
    "        X_val_event = X_val_event.astype(int)\n",
    "        X_test_resource = X_test_resource.astype(int)\n",
    "        X_train_resource = X_train_resource.astype(int)\n",
    "        X_val_resource = X_val_resource.astype(int)\n",
    "        X_test_m = X_test_m.astype(int)\n",
    "        X_train_m = X_train_m.astype(int)\n",
    "        X_val_m = X_val_m.astype(int)\n",
    "        #for combs again\n",
    "        X_test_event_c = X_test_event_c.astype(int)\n",
    "        X_train_event_c = X_train_event_c.astype(int)\n",
    "        X_val_event_c = X_val_event_c.astype(int)\n",
    "        X_test_resource_c = X_test_resource_c.astype(int)\n",
    "        X_train_resource_c = X_train_resource_c.astype(int)\n",
    "        X_val_resource_c = X_val_resource_c.astype(int)\n",
    "        X_test_m_c = X_test_m_c.astype(int)\n",
    "        X_train_m_c = X_train_m_c.astype(int)\n",
    "        X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BPDP_LSTM(vocab_events=len(events_encoder), vocab_resources=len(resource_encoder), no_TA=len(X_train_TA[0]),\n",
    "                          vocab_month=len(month_encoder))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data_event = TrainData(torch.FloatTensor(X_train_event),\n",
    "                                         torch.FloatTensor(y_train))\n",
    "            train_data_resource = TestData(torch.FloatTensor(X_train_resource))\n",
    "            train_data_TA = TestData(torch.FloatTensor(X_train_TA))\n",
    "            train_data_m = TestData(torch.FloatTensor(X_train_m))\n",
    "            train_loader_event = DataLoader(dataset=train_data_event, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_resource = DataLoader(dataset=train_data_resource, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_TA = DataLoader(dataset=train_data_TA, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_m = DataLoader(dataset=train_data_m, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader_event))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                steps_r = list((train_loader_resource))\n",
    "                steps_ta = list((train_loader_TA))\n",
    "                steps_m = list((train_loader_m))\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                         steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(torch.FloatTensor(X_val_event).to(torch.int64),\n",
    "                                     torch.FloatTensor(X_val_resource).to(torch.int64),\n",
    "                                     torch.FloatTensor(X_val_TA).to(torch.float),\n",
    "                                     torch.FloatTensor(X_val_m).to(torch.int64))\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader_event)}, Acc: {epoch_acc / len(train_loader_event):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader_event):}, Acc: {epoch_acc / len(train_loader_event):.3f}\")\n",
    "\n",
    "        y_batch_pred\n",
    "        model.eval()\n",
    "        test_data_event = TestData(torch.FloatTensor(X_test_event))\n",
    "        test_data_resource = TestData(torch.FloatTensor(X_test_resource))\n",
    "        test_data_TA = TestData(torch.FloatTensor(X_test_TA))\n",
    "        test_data_m = TestData(torch.FloatTensor(X_test_m))\n",
    "        test_loader_event = DataLoader(dataset=test_data_event, batch_size=1)\n",
    "        test_loader_resource = DataLoader(dataset=test_data_resource, batch_size=1)\n",
    "        test_loader_TA = DataLoader(dataset=test_data_TA, batch_size=1)\n",
    "        test_loader_m = DataLoader(dataset=test_data_m, batch_size=1)\n",
    "        iterations_r = iter(test_loader_resource)\n",
    "        iterations_ta = iter(test_loader_TA)\n",
    "        iterations_m = iter(test_loader_m)\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, X_batch in enumerate(test_loader_event):\n",
    "                X_batch = X_batch.to(device).to(torch.int64)\n",
    "                y_test_pred = torch.nn.functional.softmax(\n",
    "                    model(X_batch, next(iterations_r).to(torch.int64), next(iterations_ta).to(torch.float),\n",
    "                          next(iterations_m).to(torch.int64)))\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        time_elapsed = time_start - time.clock()\n",
    "        metrics[d]['Time']=time_elapsed\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        print(metrics)\n",
    "        models_collect[d] = model\n",
    "\n",
    "        X_test_event_c = X_test_event_c.astype(int)\n",
    "        X_train_event_c = X_train_event_c.astype(int)\n",
    "        X_val_event_c = X_val_event_c.astype(int)\n",
    "        X_test_resource_c = X_test_resource_c.astype(int)\n",
    "        X_train_resource_c = X_train_resource_c.astype(int)\n",
    "        X_val_resource_c = X_val_resource_c.astype(int)\n",
    "        X_test_m_c = X_test_m_c.astype(int)\n",
    "        X_train_m_c = X_train_m_c.astype(int)\n",
    "        X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "        train_data_f_combs_event = TestData(torch.FloatTensor(X_train_event_c))\n",
    "        train_loader_f_combs_event = DataLoader(dataset=train_data_f_combs_event, batch_size=len(X_train_event_c))\n",
    "        train_data_f_combs_resource = TestData(torch.FloatTensor(X_train_resource_c))\n",
    "        train_loader_f_combs_resource = DataLoader(dataset=train_data_f_combs_resource, batch_size=len(X_train_resource_c))\n",
    "        train_data_f_combs_m = TestData(torch.FloatTensor(X_train_m_c))\n",
    "        train_loader_f_combs_m = DataLoader(dataset=train_data_f_combs_m, batch_size=len(X_train_m_c))\n",
    "        train_data_f_combs_TA = TestData(torch.FloatTensor(X_train_TA_c))\n",
    "        train_loader_f_combs_TA = DataLoader(dataset=train_data_f_combs_TA, batch_size=len(X_train_TA_c))\n",
    "\n",
    "        y_output_train = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((train_loader_f_combs_resource))\n",
    "            steps_ta = list((train_loader_f_combs_TA))\n",
    "            steps_m = list((train_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(train_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_train.append(y_test_pred.numpy())\n",
    "\n",
    "        test_data_f_combs_event = TestData(torch.FloatTensor(X_test_event_c))\n",
    "        test_loader_f_combs_event = DataLoader(dataset=test_data_f_combs_event, batch_size=len(X_test_event_c))\n",
    "        test_data_f_combs_resource = TestData(torch.FloatTensor(X_test_resource_c))\n",
    "        test_loader_f_combs_resource = DataLoader(dataset=test_data_f_combs_resource, batch_size=len(X_test_resource_c))\n",
    "        test_data_f_combs_m = TestData(torch.FloatTensor(X_test_m_c))\n",
    "        test_loader_f_combs_m = DataLoader(dataset=test_data_f_combs_m, batch_size=len(X_test_m_c))\n",
    "        test_data_f_combs_TA = TestData(torch.FloatTensor(X_test_TA_c))\n",
    "        test_loader_f_combs_TA = DataLoader(dataset=test_data_f_combs_TA, batch_size=len(X_test_TA_c))\n",
    "\n",
    "        y_output_test = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((test_loader_f_combs_resource))\n",
    "            steps_ta = list((test_loader_f_combs_TA))\n",
    "            steps_m = list((test_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(test_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_test.append(y_test_pred.numpy())\n",
    "\n",
    "        val_data_f_combs_event = TestData(torch.FloatTensor(X_val_event_c))\n",
    "        val_loader_f_combs_event = DataLoader(dataset=val_data_f_combs_event, batch_size=len(X_val_event_c))\n",
    "        val_data_f_combs_resource = TestData(torch.FloatTensor(X_val_resource_c))\n",
    "        val_loader_f_combs_resource = DataLoader(dataset=val_data_f_combs_resource, batch_size=len(X_val_resource_c))\n",
    "        val_data_f_combs_m = TestData(torch.FloatTensor(X_val_m_c))\n",
    "        val_loader_f_combs_m = DataLoader(dataset=val_data_f_combs_m, batch_size=len(X_val_m_c))\n",
    "        val_data_f_combs_TA = TestData(torch.FloatTensor(X_val_TA_c))\n",
    "        val_loader_f_combs_TA = DataLoader(dataset=val_data_f_combs_TA, batch_size=len(X_val_TA_c))\n",
    "\n",
    "        y_output_val = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((val_loader_f_combs_resource))\n",
    "            steps_ta = list((val_loader_f_combs_TA))\n",
    "            steps_m = list((val_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(val_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_val.append(y_test_pred.numpy())\n",
    "\n",
    "        outputs_train['NoDev' + str(d)] = y_output_train[0][:, 0]\n",
    "        outputs_train['Dev' + str(d)] = y_output_train[0][:, 1]\n",
    "        outputs_test['NoDev' + str(d)] = y_output_test[0][:, 0]\n",
    "        outputs_test['Dev' + str(d)] = y_output_test[0][:, 1]\n",
    "        outputs_val['NoDev' + str(d)] = y_output_val[0][:, 0]\n",
    "        outputs_val['Dev' + str(d)] = y_output_val[0][:, 1]\n",
    "        if d == dev[0]:\n",
    "            outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "            outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "            outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_LSTM_CIBE(log, ref_log, aligned_traces)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_collective_LSTM_CIBE(log, ref_log, aligned_traces, u_sample = True,early_stop = True,explained = False,split = 1 / 3):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "\n",
    "    ref_enc_dat = ref_clean_dat.copy()\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.00001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = clean_dat.copy()\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 'No'  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='No')\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_LSTM')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    ref_enc_dat\n",
    "\n",
    "    evs_c = []\n",
    "    resource_c = []\n",
    "    month_c = []\n",
    "    trace_attr = []\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('event'): evs_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('resource'): resource_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('month'): month_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if not (ca in evs_c or ca in resource_c or ca in month_c): trace_attr.append(ca)\n",
    "    print(evs_c)\n",
    "    print(resource_c)\n",
    "    print(month_c)\n",
    "    print(trace_attr)\n",
    "\n",
    "    X_events = {}\n",
    "    X_resource = {}\n",
    "    X_month = {}\n",
    "    X_tracea = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        X_events[prefix] = X_cum[prefix][evs_c]\n",
    "        X_resource[prefix] = X_cum[prefix][resource_c]\n",
    "        X_month[prefix] = X_cum[prefix][month_c]\n",
    "        X_tracea[prefix] = X_cum[prefix][trace_attr]\n",
    "\n",
    "    cat_tas = []\n",
    "    for cat in X_tracea[1].columns:\n",
    "        if type(X_tracea[1][cat][0]) == str:\n",
    "            cat_tas.append(cat)\n",
    "    cat_tas\n",
    "    uniques_cats = {}\n",
    "    for cat in cat_tas:\n",
    "        uniques_cats[cat] = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        for cat in cat_tas:\n",
    "            for reals in list(X_tracea[prefix][cat].unique()):\n",
    "                if not reals in uniques_cats[cat]:\n",
    "                    uniques_cats[cat].append(reals)\n",
    "    uniques_cats\n",
    "    X_tracea[1]\n",
    "    for cat in cat_tas:\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for j in list(X_tracea[prefix].index):\n",
    "                X_tracea[prefix][cat][j] = uniques_cats[cat].index(X_tracea[prefix][cat][j])\n",
    "    X_tracea[1]\n",
    "    X_events[1]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 8\n",
    "        negative_weights[label] = 1\n",
    "    models_collect = {}\n",
    "    dev_trained = []\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "    Y_cum_dev = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "        Y_cum_dev[prefix]['NoDev'] = 0\n",
    "        for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "            Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "        if prefix == 1:\n",
    "            print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "    if u_sample:\n",
    "        imb_ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "        imb_ref_enc_dat['ind'] = 0\n",
    "        for i in range(len(imb_ref_enc_dat)):\n",
    "            imb_ref_enc_dat['ind'][i] = i\n",
    "\n",
    "        imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "        for trace in range(len(log)):\n",
    "            if dev_df.loc[trace].sum() > 0:\n",
    "                imb_traces['Dev'][trace] = 1\n",
    "\n",
    "        imb_traces = imb_traces.drop(x_test_idx)\n",
    "        imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "        x_train_idx = list(X_resampled['ind'])\n",
    "        y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "    print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "    # validation set for early stopping\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "\n",
    "                                                                      random_state=0)\n",
    "\n",
    "    enumerated_trace_idx = {}\n",
    "    cum_trace_idxs = []\n",
    "    pref_list = []\n",
    "    pref_list_train_c = []\n",
    "    pref_list_test_c = []\n",
    "    pref_list_val_c = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_events[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_events[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_events[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        y_te = y_cum_test[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_tr = y_cum_test[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va = y_cum_test[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_te_c = X_events[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_events[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_events[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "            float)\n",
    "        y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "            [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "            float)\n",
    "        cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "        pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "        enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "        pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "        pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "        pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_event = x_tr\n",
    "            X_test_event = x_te\n",
    "            y_train = y_tr\n",
    "            y_test = y_te\n",
    "            X_val_event = x_va\n",
    "            y_val = y_va\n",
    "            X_train_event_c = x_tr_c\n",
    "            X_test_event_c = x_te_c\n",
    "            X_val_event_c = x_va_c\n",
    "            y_train_c = y_tr_c\n",
    "            y_test_c = y_te_c\n",
    "            y_val_c = y_va_c\n",
    "        else:\n",
    "            X_train_event = np.append(X_train_event, x_tr, axis=0)\n",
    "            X_test_event = np.append(X_test_event, x_te, axis=0)\n",
    "            y_train = np.append(y_train, y_tr, axis=0)\n",
    "            y_test = np.append(y_test, y_te, axis=0)\n",
    "            y_val = np.append(y_val, y_va, axis=0)\n",
    "            X_val_event = np.append(X_val_event, x_va, axis=0)\n",
    "            X_train_event_c = np.append(X_train_event_c, x_tr_c, axis=0)\n",
    "            X_test_event_c = np.append(X_test_event_c, x_te_c, axis=0)\n",
    "            X_val_event_c = np.append(X_val_event_c, x_va_c, axis=0)\n",
    "            y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "            y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "            y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_event), len(y_train), len(y_train_c), len(X_val_event), len(y_val), len(y_val_c),\n",
    "          len(X_test_event), len(y_test), len(y_test_c))\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_resource[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_resource[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_resource[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_resource[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_resource[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_resource[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_resource = x_tr\n",
    "            X_test_resource = x_te\n",
    "            X_val_resource = x_va\n",
    "            X_train_resource_c = x_tr_c\n",
    "            X_test_resource_c = x_te_c\n",
    "            X_val_resource_c = x_va_c\n",
    "        else:\n",
    "            X_train_resource = np.append(X_train_resource, x_tr, axis=0)\n",
    "            X_test_resource = np.append(X_test_resource, x_te, axis=0)\n",
    "            X_val_resource = np.append(X_val_resource, x_va, axis=0)\n",
    "            X_train_resource_c = np.append(X_train_resource_c, x_tr_c, axis=0)\n",
    "            X_test_resource_c = np.append(X_test_resource_c, x_te_c, axis=0)\n",
    "            X_val_resource_c = np.append(X_val_resource_c, x_va_c,\n",
    "                                         axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_resource), len(y_train), len(X_val_resource), len(y_val), len(X_test_resource), len(y_test))\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_month[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_month[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_month[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_month[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_month[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_month[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_m = x_tr\n",
    "            X_test_m = x_te\n",
    "            X_val_m = x_va\n",
    "            X_train_m_c = x_tr_c\n",
    "            X_test_m_c = x_te_c\n",
    "            X_val_m_c = x_va_c\n",
    "        else:\n",
    "            X_train_m = np.append(X_train_m, x_tr, axis=0)\n",
    "            X_test_m = np.append(X_test_m, x_te, axis=0)\n",
    "            X_val_m = np.append(X_val_m, x_va, axis=0)\n",
    "            X_train_m_c = np.append(X_train_m_c, x_tr_c, axis=0)\n",
    "            X_test_m_c = np.append(X_test_m_c, x_te_c, axis=0)\n",
    "            X_val_m_c = np.append(X_val_m_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_m), len(y_train), len(X_val_m), len(y_val), len(X_test_m), len(y_test))\n",
    "\n",
    "    print('split done')\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_tracea[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_tracea[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_tracea[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_tracea[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_tracea[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_tracea[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_TA = x_tr\n",
    "            X_test_TA = x_te\n",
    "            X_val_TA = x_va\n",
    "            X_train_TA_c = x_tr_c\n",
    "            X_test_TA_c = x_te_c\n",
    "            X_val_TA_c = x_va_c\n",
    "        else:\n",
    "            X_train_TA = np.append(X_train_TA, x_tr, axis=0)\n",
    "            X_test_TA = np.append(X_test_TA, x_te, axis=0)\n",
    "            X_val_TA = np.append(X_val_TA, x_va, axis=0)\n",
    "            X_train_TA_c = np.append(X_train_TA_c, x_tr_c, axis=0)\n",
    "            X_test_TA_c = np.append(X_test_TA_c, x_te_c, axis=0)\n",
    "            X_val_TA_c = np.append(X_val_TA_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_TA), len(y_train), len(X_val_TA), len(y_val), len(X_test_TA), len(y_test))\n",
    "\n",
    "    events_encoder = list(\n",
    "        np.unique(np.append(np.append(X_train_event_c, X_test_event_c, axis=0), X_val_event_c, axis=0)))\n",
    "    events_encoder.index('No')\n",
    "    resource_encoder = list(\n",
    "        np.unique(np.append(np.append(X_train_resource_c, X_test_resource_c, axis=0), X_val_resource_c, axis=0)))\n",
    "    resource_encoder.index('No')\n",
    "    cat_ecnoders = {}\n",
    "    for cat in cat_tas:\n",
    "        cat_ecnoders[cat] = list(np.unique(np.append(np.append(X_train_TA_c, X_test_TA_c, axis=0), X_val_TA_c, axis=0)))\n",
    "\n",
    "    month_encoder = list(np.unique(np.append(np.append(X_train_m_c, X_test_m_c, axis=0), X_val_m_c, axis=0)))\n",
    "    month_encoder\n",
    "    for i in range(len(X_test_event)):\n",
    "        for j in range(len(X_test_event[0])):\n",
    "            X_test_event[i][j] = events_encoder.index(X_test_event[i][j])\n",
    "        for j in range(len(X_test_resource[0])):\n",
    "            X_test_resource[i][j] = resource_encoder.index(X_test_resource[i][j])\n",
    "        for j in range(len(X_test_m[0])):\n",
    "            X_test_m[i][j] = month_encoder.index(X_test_m[i][j])\n",
    "    for i in range(len(X_train_event)):\n",
    "        for j in range(len(X_train_event[0])):\n",
    "            X_train_event[i][j] = events_encoder.index(X_train_event[i][j])\n",
    "        for j in range(len(X_train_resource[0])):\n",
    "            X_train_resource[i][j] = resource_encoder.index(X_train_resource[i][j])\n",
    "        for j in range(len(X_train_m[0])):\n",
    "            X_train_m[i][j] = month_encoder.index(X_train_m[i][j])\n",
    "    for i in range(len(X_val_event)):\n",
    "        for j in range(len(X_val_event[0])):\n",
    "            X_val_event[i][j] = events_encoder.index(X_val_event[i][j])\n",
    "        for j in range(len(X_val_resource[0])):\n",
    "            X_val_resource[i][j] = resource_encoder.index(X_val_resource[i][j])\n",
    "        for j in range(len(X_val_m[0])):\n",
    "            X_val_m[i][j] = month_encoder.index(X_val_m[i][j])\n",
    "\n",
    "    # for combs_output\n",
    "    for i in range(len(X_test_event_c)):\n",
    "        for j in range(len(X_test_event_c[0])):\n",
    "            X_test_event_c[i][j] = events_encoder.index(X_test_event_c[i][j])\n",
    "        for j in range(len(X_test_resource[0])):\n",
    "            X_test_resource_c[i][j] = resource_encoder.index(X_test_resource_c[i][j])\n",
    "        for j in range(len(X_test_m[0])):\n",
    "            X_test_m_c[i][j] = month_encoder.index(X_test_m_c[i][j])\n",
    "    for i in range(len(X_train_event_c)):\n",
    "        for j in range(len(X_train_event_c[0])):\n",
    "            X_train_event_c[i][j] = events_encoder.index(X_train_event_c[i][j])\n",
    "        for j in range(len(X_train_resource[0])):\n",
    "            X_train_resource_c[i][j] = resource_encoder.index(X_train_resource_c[i][j])\n",
    "        for j in range(len(X_train_m[0])):\n",
    "            X_train_m_c[i][j] = month_encoder.index(X_train_m_c[i][j])\n",
    "    for i in range(len(X_val_event_c)):\n",
    "        for j in range(len(X_val_event_c[0])):\n",
    "            X_val_event_c[i][j] = events_encoder.index(X_val_event_c[i][j])\n",
    "        for j in range(len(X_val_resource_c[0])):\n",
    "            X_val_resource_c[i][j] = resource_encoder.index(X_val_resource_c[i][j])\n",
    "        for j in range(len(X_val_m_c[0])):\n",
    "            X_val_m_c[i][j] = month_encoder.index(X_val_m_c[i][j])\n",
    "    scaler = StandardScaler()\n",
    "    X_test_TA = scaler.fit_transform(X_test_TA)\n",
    "    X_train_TA = scaler.fit_transform(X_train_TA)\n",
    "    X_val_TA = scaler.fit_transform(X_val_TA)\n",
    "    X_test_TA_c = scaler.fit_transform(X_test_TA_c)\n",
    "    X_train_TA_c = scaler.fit_transform(X_train_TA_c)\n",
    "    X_val_TA_c = scaler.fit_transform(X_val_TA_c)\n",
    "    X_test_event = X_test_event.astype(int)\n",
    "    X_train_event = X_train_event.astype(int)\n",
    "    X_val_event = X_val_event.astype(int)\n",
    "    X_test_resource = X_test_resource.astype(int)\n",
    "    X_train_resource = X_train_resource.astype(int)\n",
    "    X_val_resource = X_val_resource.astype(int)\n",
    "    X_test_m = X_test_m.astype(int)\n",
    "    X_train_m = X_train_m.astype(int)\n",
    "    X_val_m = X_val_m.astype(int)\n",
    "    #for combs again\n",
    "    X_test_event_c = X_test_event_c.astype(int)\n",
    "    X_train_event_c = X_train_event_c.astype(int)\n",
    "    X_val_event_c = X_val_event_c.astype(int)\n",
    "    X_test_resource_c = X_test_resource_c.astype(int)\n",
    "    X_train_resource_c = X_train_resource_c.astype(int)\n",
    "    X_val_resource_c = X_val_resource_c.astype(int)\n",
    "    X_test_m_c = X_test_m_c.astype(int)\n",
    "    X_train_m_c = X_train_m_c.astype(int)\n",
    "    X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "    y_train\n",
    "\n",
    "\n",
    "    class BPDP_LSTM_SC(nn.Module):\n",
    "        def __init__(self, vocab_events, vocab_resources, no_TA, vocab_month, no_devs):\n",
    "            super(BPDP_LSTM_SC, self).__init__()\n",
    "            self.embedding_e = nn.Embedding(vocab_events, 16)  # hier auf 8 / 16\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.lstm_e = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_e = nn.Linear(64, 32)\n",
    "            self.embedding_r = nn.Embedding(vocab_resources, 16)\n",
    "            self.lstm_r = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_r = nn.Linear(64, 32)\n",
    "            self.embedding_m = nn.Embedding(vocab_month, 16)\n",
    "            self.lstm_m = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_m = nn.Linear(64, 32)\n",
    "            self.linear_ta = nn.Linear(no_TA, 32)\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(128)\n",
    "            self.linear = nn.Linear(128, no_devs)\n",
    "\n",
    "        def forward(self, evs, rs, tas, ms):\n",
    "            evs = self.embedding_e(evs)\n",
    "            evs, _ = self.lstm_e(evs)\n",
    "            evs = self.linear_e(evs)\n",
    "            evs = evs[:, -1, :]\n",
    "            evs = self.activation1(evs)\n",
    "            rs = self.embedding_r(rs)\n",
    "            rs, _ = self.lstm_r(rs)\n",
    "            rs = rs[:, -1, :]\n",
    "            rs = self.activation1(rs)\n",
    "            rs = self.linear_r(rs)\n",
    "            ms = self.embedding_m(ms)\n",
    "            ms, _ = self.lstm_m(ms)\n",
    "            ms = ms[:, -1, :]\n",
    "            ms = self.activation1(ms)\n",
    "            ms = self.linear_m(ms)\n",
    "            tas = self.linear_ta(tas)\n",
    "            fin = torch.cat((evs, rs), dim=1)\n",
    "            fin = torch.cat((fin, ms), dim=1)\n",
    "            fin = torch.cat((fin, tas), dim=1)\n",
    "            fin = self.batchnorm1(fin)\n",
    "            #fin = self.dropout(fin)\n",
    "            fin = self.linear(fin)\n",
    "            return fin\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for label in labels:\n",
    "        positives[label] = sum(dev_df[label] == 1)\n",
    "        negatives[label] = sum(dev_df[label] == 0)\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in labels:\n",
    "        pir[label] = max(positives[label], negatives[label]) / positives[label]\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = mean(pir.values()) ** (1 / (2 * math.e)) + np.log(pirlbl[label])\n",
    "        negative_weights[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    positive_weights\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BPDP_LSTM_SC(vocab_events=len(events_encoder), vocab_resources=len(resource_encoder), no_TA=len(X_train_TA[0]),\n",
    "                         vocab_month=len(month_encoder), no_devs=len(y_train[0]))\n",
    "    model.to(device)\n",
    "    weights = torch.FloatTensor(list(positive_weights.values()))\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    if early_stop:\n",
    "        EPOCHS = 100\n",
    "        model.train()\n",
    "        train_data_event = TrainData(torch.FloatTensor(X_train_event),\n",
    "                                     torch.FloatTensor(y_train))\n",
    "        train_data_resource = TestData(torch.FloatTensor(X_train_resource))\n",
    "        train_data_TA = TestData(torch.FloatTensor(X_train_TA))\n",
    "        train_data_m = TestData(torch.FloatTensor(X_train_m))\n",
    "        train_loader_event = DataLoader(dataset=train_data_event, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_resource = DataLoader(dataset=train_data_resource, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_TA = DataLoader(dataset=train_data_TA, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_m = DataLoader(dataset=train_data_m, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        es = EarlyStopping()\n",
    "        done = False\n",
    "\n",
    "        epoch = 0\n",
    "        while epoch < EPOCHS and not done:\n",
    "            epoch += 1\n",
    "            steps = list(enumerate(train_loader_event))\n",
    "            pbar = tqdm.tqdm(steps)\n",
    "            steps_r = list((train_loader_resource))\n",
    "            steps_ta = list((train_loader_TA))\n",
    "            steps_m = list((train_loader_m))\n",
    "            model.train()\n",
    "            epoch_acc = 0\n",
    "            epoch_loss = 0\n",
    "            for i, (x_batch, y_batch) in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                y_batch_pred = model(x_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                     steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "\n",
    "                loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "                loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                epoch_loss += loss\n",
    "                if i == len(steps) - 1:\n",
    "                    model.eval()\n",
    "                    pred = model(torch.FloatTensor(X_val_event).to(torch.int64),\n",
    "                                 torch.FloatTensor(X_val_resource).to(torch.int64),\n",
    "                                 torch.FloatTensor(X_val_TA).to(torch.float),\n",
    "                                 torch.FloatTensor(X_val_m).to(torch.int64))\n",
    "                    vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                    if es(model, vloss): done = True\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader_event)}, Acc: {epoch_acc / len(train_loader_event):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                else:\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader_event):}, Acc: {epoch_acc / len(train_loader_event):.3f}\")\n",
    "\n",
    "    y_batch_pred\n",
    "    model.eval()\n",
    "    test_data_event = TestData(torch.FloatTensor(X_test_event))\n",
    "    test_data_resource = TestData(torch.FloatTensor(X_test_resource))\n",
    "    test_data_TA = TestData(torch.FloatTensor(X_test_TA))\n",
    "    test_data_m = TestData(torch.FloatTensor(X_test_m))\n",
    "    test_loader_event = DataLoader(dataset=test_data_event, batch_size=1)\n",
    "    test_loader_resource = DataLoader(dataset=test_data_resource, batch_size=1)\n",
    "    test_loader_TA = DataLoader(dataset=test_data_TA, batch_size=1)\n",
    "    test_loader_m = DataLoader(dataset=test_data_m, batch_size=1)\n",
    "    iterations_r = iter(test_loader_resource)\n",
    "    iterations_ta = iter(test_loader_TA)\n",
    "    iterations_m = iter(test_loader_m)\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, X_batch in enumerate(test_loader_event):\n",
    "            X_batch = X_batch.to(device).to(torch.int64)\n",
    "            y_test_pred = torch.nn.functional.sigmoid(\n",
    "                model(X_batch, next(iterations_r).to(torch.int64), next(iterations_ta).to(torch.float),\n",
    "                      next(iterations_m).to(torch.int64)))\n",
    "            y_pred_tag = torch.round(y_test_pred)\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    for i, d in enumerate(dev):\n",
    "        metrics[d]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[d]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        metrics[d]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                  average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    print(CM)\n",
    "\n",
    "    print(metrics)\n",
    "    writer = pd.ExcelWriter('BPDP_LSTM/' + z + '_BPDP_LSTM_SC_1.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_DPP_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    max_combs_l\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "    max_combs_l\n",
    "    test_counts\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "    max_combs\n",
    "    y_cum_test[1]\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    dev_df.loc[x_test_idx].sum()[max_combs_l[0][0]]\n",
    "    pi = 4\n",
    "    print(y_cum_test_combs[pi].sum())\n",
    "    print(y_cum_test[pi].sum())\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "    y_cum_test_combs[1]\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_Classifier')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "    models_collect = {}\n",
    "    dev_trained = []\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_te_c = X_cum[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr_c = X_cum[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va_c = X_cum[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "                [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "            pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "                X_train_c = x_tr_c\n",
    "                X_test_c = x_te_c\n",
    "                X_val_c = x_va_c\n",
    "                y_train_c = y_tr_c\n",
    "                y_test_c = y_te_c\n",
    "                y_val_c = y_va_c\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)\n",
    "                X_train_c = np.append(X_train_c, x_tr_c, axis=0)\n",
    "                X_test_c = np.append(X_test_c, x_te_c, axis=0)\n",
    "                X_val_c = np.append(X_val_c, x_va_c, axis=0)\n",
    "                y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "                y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "                y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(y_train_c), len(X_val), len(y_val), len(y_val_c), len(X_test), len(y_test),\n",
    "              len(y_test_c))\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "        X_test_c = scaler.fit_transform(X_test_c)\n",
    "        X_train_c = scaler.fit_transform(X_train_c)\n",
    "        X_val_c = scaler.fit_transform(X_val_c)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                   torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        cum_trace_idxs = flatten_comprehension(cum_trace_idxs)\n",
    "        pref_list = flatten_comprehension(pref_list)\n",
    "        if d == dev[0]:\n",
    "            y_test_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_test_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_test_cum['prefix_length'] = pref_list\n",
    "            y_pred_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_pred_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_pred_cum['prefix_length'] = pref_list\n",
    "        y_pred_cum[str('confidence' + d)] = y_confidence_list\n",
    "        y_test_cum[d] = list(y_test)\n",
    "        y_pred_cum[d] = y_pred_list\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        print(metrics)\n",
    "        models_collect[d] = model\n",
    "\n",
    "        train_data_f_combs = TestData(torch.FloatTensor(X_train_c))\n",
    "        train_loader_f_combs = DataLoader(dataset=train_data_f_combs, batch_size=len(X_train_c))\n",
    "\n",
    "        y_output_train = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in train_loader_f_combs:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_output_train.append(y_test_pred.numpy())\n",
    "\n",
    "        test_data_f_combs = TestData(torch.FloatTensor(X_test_c))\n",
    "        test_loader_f_combs = DataLoader(dataset=test_data_f_combs, batch_size=len(X_test_c))\n",
    "\n",
    "        y_output_test = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader_f_combs:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_output_test.append(y_test_pred.numpy())\n",
    "\n",
    "        val_data_f_combs = TestData(torch.FloatTensor(X_val_c))\n",
    "        val_loader_f_combs = DataLoader(dataset=val_data_f_combs, batch_size=len(X_val_c))\n",
    "\n",
    "        y_output_val = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in val_loader_f_combs:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_output_val.append(y_test_pred.numpy())\n",
    "\n",
    "        outputs_train['NoDev' + str(d)] = y_output_train[0][:, 0]\n",
    "        outputs_train['Dev' + str(d)] = y_output_train[0][:, 1]\n",
    "        outputs_test['NoDev' + str(d)] = y_output_test[0][:, 0]\n",
    "        outputs_test['Dev' + str(d)] = y_output_test[0][:, 1]\n",
    "        outputs_val['NoDev' + str(d)] = y_output_val[0][:, 0]\n",
    "        outputs_val['Dev' + str(d)] = y_output_val[0][:, 1]\n",
    "        if d == dev[0]:\n",
    "            outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "            outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "            outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "    outputs_test\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()\n",
    "    y_test_c.sum()\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    metrics_comb\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs(nn.Module):\n",
    "        def __init__(self, no_columns, no_devs):\n",
    "            super(Ensemble_Stack_Combs, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 64)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(128, 64)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(64, no_devs)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(64)\n",
    "            self.batchnorm2 = nn.LayerNorm(64)\n",
    "            self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.layer_1(inputs)\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            #x = self.layer_2(x)\n",
    "            #x = self.activation2(self.layer_2(x))\n",
    "            #x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            #x = self.Sigmoid(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs_Single(nn.Module):\n",
    "        def __init__(self, no_columns):\n",
    "            super(Ensemble_Stack_Combs_Single, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 512)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(512, 128)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(128, 2)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(512)\n",
    "            self.batchnorm2 = nn.LayerNorm(128)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.activation2(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    len(y_train_c)\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for i, label in enumerate(unique_rel_combs):\n",
    "        positives[label] = y_train_c[:, i].sum()\n",
    "        negatives[label] = len(y_train_c) - y_train_c[:, i].sum()\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pir[label] = min((max(positives[label], negatives[label]) / positives[label]), 10000)\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    pw_combs = {}\n",
    "    nw_combs = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pw_combs[label] = min((mean(pir.values()) ** (4 / (2 ** math.e)) + (np.log(pirlbl[label]))), 200)\n",
    "        #pw_combs[label] =  min(2*(mean(pir.values()) ** ((2* math.e)) + (np.log(pirlbl[label]))), 100000)\n",
    "        nw_combs[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    pw_combs\n",
    "\n",
    "    if len(y_train_c[0]) == 1:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "\n",
    "        model = Ensemble_Stack_Combs_Single(no_columns=len(outputs_train_us[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor([8, 1])\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            y_train_c_us['NoDev']=0\n",
    "            for je in list(y_train_c_us.index):\n",
    "                y_train_c_us['NoDev'][je]=1-y_train_c_us[i][je]\n",
    "            y_val = np.array(y_val_c[:, i], )\n",
    "            y_val = np.column_stack((y_val, 1 - y_val))\n",
    "            y_test = np.array(y_test_c[:, i], )\n",
    "            y_test = np.column_stack((y_test, 1 - y_test))\n",
    "\n",
    "            if early_stop:\n",
    "                EPOCHS = 300\n",
    "                model.train()\n",
    "                train_data = TrainData(torch.FloatTensor(outputs_train_us),\n",
    "                                       torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "\n",
    "                train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "                X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "                es = EarlyStopping()\n",
    "                done = False\n",
    "\n",
    "                epoch = 0\n",
    "                while epoch < EPOCHS and not done:\n",
    "                    epoch += 1\n",
    "                    steps = list(enumerate(train_loader))\n",
    "                    pbar = tqdm.tqdm(steps)\n",
    "                    model.train()\n",
    "                    epoch_acc = 0\n",
    "                    epoch_loss = 0\n",
    "                    for i, (x_batch, y_batch) in pbar:\n",
    "                        optimizer.zero_grad()\n",
    "                        y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                        loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                        acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_acc += acc.item()\n",
    "\n",
    "                        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                        epoch_loss += loss\n",
    "                        if i == len(steps) - 1:\n",
    "                            model.eval()\n",
    "                            pred = model(X_val)\n",
    "                            vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                            if es(model, vloss): done = True\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                        else:\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "            model.eval()\n",
    "            test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "            test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "            y_pred_list = []\n",
    "            y_confidence_list = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch in test_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                    y_confidence_list.append(y_test_pred.numpy())\n",
    "                    y_pred_tag = torch.round(y_test_pred)\n",
    "                    y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "            y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "            print(CM)\n",
    "\n",
    "            metrics_comb[urc]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics_comb[urc]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "    else:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "        model = Ensemble_Stack_Combs(no_columns=len(outputs_train_us[0]),\n",
    "                                     no_devs=len(y_train_c_us.loc[list(y_train_c_us.index)[0]]))\n",
    "        #model = Ensemble_Stack_Combs(no_columns=len(outputs_train.loc[0]), no_devs=len(y_train_c[0]))\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(list(pw_combs.values())))\n",
    "        #criterion = nn.MultiLabelSoftMarginLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(outputs_train_us), torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "            #train_data = TrainData(torch.FloatTensor(outputs_train.to_numpy()),torch.FloatTensor(y_train_c))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val_c))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.sigmoid(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "        print(CM)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "            metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                             average='macro')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    metrics_comb\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_BPDP_stacked_CIBE_combinations_FFN_w32.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_DPP_CIBE(log, ref_log, aligned_traces)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_collective_DPP_CIBE(log, ref_log, aligned_traces, u_sample=True, split=1/3, early_stop=True,relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    max_combs_l\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "    max_combs_l\n",
    "    test_counts\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "    max_combs\n",
    "    y_cum_test[1]\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    dev_df.loc[x_test_idx].sum()[max_combs_l[0][0]]\n",
    "    pi = 4\n",
    "    print(y_cum_test_combs[pi].sum())\n",
    "    print(y_cum_test[pi].sum())\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "    y_cum_test_combs[1]\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'Support', 'ROC_AUC', 'LenTrain', 'LenTrain_beforeUS_0',\n",
    "                                  'LenTrain_beforeUS_1', 'LenTrain_afterUS_0', 'LenTrain_afterUS_1'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    min_pref = 1\n",
    "    max_pref = max_ev\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    N = len(dev_df)\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for label in labels:\n",
    "        positives[label] = sum(dev_df[label] == 1)\n",
    "        negatives[label] = sum(dev_df[label] == 0)\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in labels:\n",
    "        pir[label] = max(positives[label], negatives[label]) / positives[label]\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = mean(pir.values()) ** (1 / (4 * math.e)) + np.log(pirlbl[label])\n",
    "        negative_weights[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "        metrics[d]['LenTrain_beforeUS_1'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        metrics[d]['LenTrain_beforeUS_0'] = len(x_train_idx) - sum(dev_df[d][i] for i in x_train_idx)\n",
    "\n",
    "    if u_sample:\n",
    "        imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "        imb_ref_enc_dat['ind'] = 0\n",
    "        for i in range(len(imb_ref_enc_dat)):\n",
    "            imb_ref_enc_dat['ind'][i] = i\n",
    "\n",
    "        imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "        for trace in range(len(log)):\n",
    "            if dev_df.loc[trace].sum() > 0:\n",
    "                imb_traces['Dev'][trace] = 1\n",
    "        imb_traces = imb_traces.drop(x_test_idx)\n",
    "        imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "        X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "        x_train_idx = list(X_resampled['ind'])\n",
    "        y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "    devq_df = dev_df.loc[x_train_idx]\n",
    "    print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "    for d in dev:\n",
    "        metrics[d]['LenTrain'] = len(x_train_idx)\n",
    "        metrics[d]['LenTrain_afterUS_1'] = devq_df[d].sum()\n",
    "        metrics[d]['LenTrain_afterUS_0'] = len(x_train_idx) - devq_df[d].sum()\n",
    "    # validation set for early stopping\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                      random_state=0)\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    enumerated_trace_idx = {}\n",
    "    cum_trace_idxs = []\n",
    "    pref_list = []\n",
    "    pref_list_train_c = []\n",
    "    pref_list_test_c = []\n",
    "    pref_list_val_c = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_te = y_cum_test[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va = y_cum_test[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_tr = y_cum_test[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_te_c = X_cum[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_tr_c = X_cum[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_va_c = X_cum[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "            float)\n",
    "        y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "            [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "            float)\n",
    "        cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "        pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "        pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "        pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "        pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "        enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train = x_tr\n",
    "            X_test = x_te\n",
    "            y_train = y_tr\n",
    "            y_test = y_te\n",
    "            X_val = x_va\n",
    "            y_val = y_va\n",
    "            X_train_c = x_tr_c\n",
    "            X_test_c = x_te_c\n",
    "            X_val_c = x_va_c\n",
    "            y_train_c = y_tr_c\n",
    "            y_test_c = y_te_c\n",
    "            y_val_c = y_va_c\n",
    "        else:\n",
    "            X_train = np.append(X_train, x_tr, axis=0)\n",
    "            X_test = np.append(X_test, x_te, axis=0)\n",
    "            y_train = np.append(y_train, y_tr, axis=0)\n",
    "            y_test = np.append(y_test, y_te, axis=0)\n",
    "            y_val = np.append(y_val, y_va, axis=0)\n",
    "            X_val = np.append(X_val, x_va, axis=0)\n",
    "            X_train_c = np.append(X_train_c, x_tr_c, axis=0)\n",
    "            X_test_c = np.append(X_test_c, x_te_c, axis=0)\n",
    "            X_val_c = np.append(X_val_c, x_va_c, axis=0)\n",
    "            y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "            y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "            y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "    print('split done')\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "    X_test_c = scaler.fit_transform(X_test_c)\n",
    "    X_train_c = scaler.fit_transform(X_train_c)\n",
    "    X_val_c = scaler.fit_transform(X_val_c)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BinaryClassification(no_columns=len(ref_enc_dat.loc[0]), no_devs=len(dev))\n",
    "    model.to(device)\n",
    "    criterion = nn.MultiLabelSoftMarginLoss(weight=torch.FloatTensor(list(positive_weights.values())))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if early_stop:\n",
    "        print('training start with ES')\n",
    "        model.train()\n",
    "        train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                               torch.FloatTensor(y_train))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "        es = EarlyStopping()\n",
    "        done = False\n",
    "\n",
    "        epoch = 0\n",
    "        while epoch < EPOCHS and not done:\n",
    "            epoch += 1\n",
    "            steps = list(enumerate(train_loader))\n",
    "            pbar = tqdm.tqdm(steps)\n",
    "            model.train()\n",
    "            epoch_acc = 0\n",
    "            epoch_loss = 0\n",
    "            for i, (x_batch, y_batch) in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                #print(y_batch_pred.shape)\n",
    "\n",
    "                loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_acc += acc.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                current = (i + 1) * len(x_batch)\n",
    "                if i == len(steps) - 1:\n",
    "                    model.eval()\n",
    "                    pred = model(X_val)\n",
    "                    vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                    if es(model, vloss): done = True\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                else:\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_data = TestData(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_pred_tag = torch.round(torch.sigmoid_(y_test_pred))\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                       average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_Classifier')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_BPDP_single_classifier_testcounts' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()\n",
    "    cum_trace_idxs = flatten_comprehension(cum_trace_idxs)\n",
    "    pref_list = flatten_comprehension(pref_list)\n",
    "    y_test_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "    y_test_cum['trace_idx'] = cum_trace_idxs\n",
    "    y_test_cum['prefix_length'] = pref_list\n",
    "    y_pred_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "    y_pred_cum['trace_idx'] = cum_trace_idxs\n",
    "    y_pred_cum['prefix_length'] = pref_list\n",
    "\n",
    "    y_test[:, 2]\n",
    "    for i in range(len(dev)):\n",
    "        y_test_cum[dev[i]] = list(y_test[:, i])\n",
    "        y_pred_cum[dev[i]] = list(np.array(y_pred_list)[:, i])\n",
    "    print(metrics)\n",
    "\n",
    "    train_data_f_combs = TestData(torch.FloatTensor(X_train_c))\n",
    "    train_loader_f_combs = DataLoader(dataset=train_data_f_combs, batch_size=len(X_train_c))\n",
    "\n",
    "    y_output_train = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in train_loader_f_combs:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "            y_output_train.append(y_test_pred.numpy())\n",
    "\n",
    "    test_data_f_combs = TestData(torch.FloatTensor(X_test_c))\n",
    "    test_loader_f_combs = DataLoader(dataset=test_data_f_combs, batch_size=len(X_test_c))\n",
    "\n",
    "    y_output_test = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader_f_combs:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "            y_output_test.append(y_test_pred.numpy())\n",
    "\n",
    "    val_data_f_combs = TestData(torch.FloatTensor(X_val_c))\n",
    "    val_loader_f_combs = DataLoader(dataset=val_data_f_combs, batch_size=len(X_val_c))\n",
    "\n",
    "    y_output_val = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in val_loader_f_combs:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "            y_output_val.append(y_test_pred.numpy())\n",
    "\n",
    "    y_output_val[0][:, 0]\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "    for i in range(len(dev)):\n",
    "        outputs_train['Dev' + str(dev[i])] = y_output_train[0][:, i]\n",
    "        outputs_train['NoDev' + str(dev[i])] = 1 - outputs_train['Dev' + str(dev[i])]\n",
    "        outputs_test['Dev' + str(dev[i])] = y_output_test[0][:, i]\n",
    "        outputs_test['NoDev' + str(dev[i])] = 1 - outputs_test['Dev' + str(dev[i])]\n",
    "        outputs_val['Dev' + str(dev[i])] = y_output_val[0][:, i]\n",
    "        outputs_val['NoDev' + str(dev[i])] = 1 - outputs_val['Dev' + str(dev[i])]\n",
    "        outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "        outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "        outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "    outputs_test\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    metrics_comb\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs(nn.Module):\n",
    "        def __init__(self, no_columns, no_devs):\n",
    "            super(Ensemble_Stack_Combs, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 64)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(128, 64)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(64, no_devs)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(64)\n",
    "            self.batchnorm2 = nn.LayerNorm(64)\n",
    "            self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.layer_1(inputs)\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            #x = self.layer_2(x)\n",
    "            #x = self.activation2(self.layer_2(x))\n",
    "            #x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            #x = self.Sigmoid(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs_Single(nn.Module):\n",
    "        def __init__(self, no_columns):\n",
    "            super(Ensemble_Stack_Combs_Single, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 512)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(512, 128)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(128, 2)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(512)\n",
    "            self.batchnorm2 = nn.LayerNorm(128)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.activation2(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    len(y_train_c)\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for i, label in enumerate(unique_rel_combs):\n",
    "        positives[label] = y_train_c[:, i].sum()\n",
    "        negatives[label] = len(y_train_c) - y_train_c[:, i].sum()\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pir[label] = min((max(positives[label], negatives[label]) / positives[label]), 10000)\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    pw_combs = {}\n",
    "    nw_combs = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pw_combs[label] = min((mean(pir.values()) ** (4 / (2 ** math.e)) + (np.log(pirlbl[label]))), 200)\n",
    "        #pw_combs[label] =  min(2*(mean(pir.values()) ** ((2* math.e)) + (np.log(pirlbl[label]))), 100000)\n",
    "        nw_combs[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    pw_combs\n",
    "\n",
    "    if len(y_train_c[0]) == 1:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "\n",
    "        model = Ensemble_Stack_Combs_Single(no_columns=len(outputs_train_us[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor([8, 1])\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            y_train_c_us['NoDev'] = 0\n",
    "            for je in list(y_train_c_us.index):\n",
    "                y_train_c_us['NoDev'][je] = 1 - y_train_c_us[i][je]\n",
    "            y_val = np.array(y_val_c[:, i], )\n",
    "            y_val = np.column_stack((y_val, 1 - y_val))\n",
    "            y_test = np.array(y_test_c[:, i], )\n",
    "            y_test = np.column_stack((y_test, 1 - y_test))\n",
    "\n",
    "            if early_stop:\n",
    "                EPOCHS = 300\n",
    "                model.train()\n",
    "                train_data = TrainData(torch.FloatTensor(outputs_train_us),\n",
    "                                       torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "\n",
    "                train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "                X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "                es = EarlyStopping()\n",
    "                done = False\n",
    "\n",
    "                epoch = 0\n",
    "                while epoch < EPOCHS and not done:\n",
    "                    epoch += 1\n",
    "                    steps = list(enumerate(train_loader))\n",
    "                    pbar = tqdm.tqdm(steps)\n",
    "                    model.train()\n",
    "                    epoch_acc = 0\n",
    "                    epoch_loss = 0\n",
    "                    for i, (x_batch, y_batch) in pbar:\n",
    "                        optimizer.zero_grad()\n",
    "                        y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                        loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                        acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_acc += acc.item()\n",
    "\n",
    "                        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                        epoch_loss += loss\n",
    "                        if i == len(steps) - 1:\n",
    "                            model.eval()\n",
    "                            pred = model(X_val)\n",
    "                            vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                            if es(model, vloss): done = True\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                        else:\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "            model.eval()\n",
    "            test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "            test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "            y_pred_list = []\n",
    "            y_confidence_list = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch in test_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                    y_confidence_list.append(y_test_pred.numpy())\n",
    "                    y_pred_tag = torch.round(y_test_pred)\n",
    "                    y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "            y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "            print(CM)\n",
    "\n",
    "            metrics_comb[urc]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics_comb[urc]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "    else:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "        model = Ensemble_Stack_Combs(no_columns=len(outputs_train_us[0]),\n",
    "                                     no_devs=len(y_train_c_us.loc[list(y_train_c_us.index)[0]]))\n",
    "        #model = Ensemble_Stack_Combs(no_columns=len(outputs_train.loc[0]), no_devs=len(y_train_c[0]))\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(list(pw_combs.values())))\n",
    "        #criterion = nn.MultiLabelSoftMarginLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(outputs_train_us), torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "            #train_data = TrainData(torch.FloatTensor(outputs_train.to_numpy()),torch.FloatTensor(y_train_c))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val_c))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.sigmoid(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "        print(CM)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "            metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                             average='macro')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    metrics_comb\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_BPDP_stacked_CIBE_DPP_collective.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_collective_DPP_CIBE(log, ref_log, aligned_traces, u_sample=True, split=1/3, early_stop=True,relevance_ths = .5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_LSTM_DPP_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "\n",
    "    ref_enc_dat = ref_clean_dat.copy()\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.00001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC','Time'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = clean_dat.copy()\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 'No'  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='No')\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_LSTM')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter('BPDP_LSTM' + '/' + z + '_BPDP_LSTM_time_stopped.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    ref_enc_dat\n",
    "\n",
    "    evs_c = []\n",
    "    resource_c = []\n",
    "    month_c = []\n",
    "    trace_attr = []\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('event'): evs_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('resource'): resource_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('month'): month_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if not (ca in evs_c or ca in resource_c or ca in month_c): trace_attr.append(ca)\n",
    "    print(evs_c)\n",
    "    print(resource_c)\n",
    "    print(month_c)\n",
    "    print(trace_attr)\n",
    "\n",
    "    X_events = {}\n",
    "    X_resource = {}\n",
    "    X_month = {}\n",
    "    X_tracea = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        X_events[prefix] = X_cum[prefix][evs_c]\n",
    "        X_resource[prefix] = X_cum[prefix][resource_c]\n",
    "        X_month[prefix] = X_cum[prefix][month_c]\n",
    "        X_tracea[prefix] = X_cum[prefix][trace_attr]\n",
    "\n",
    "    cat_tas = []\n",
    "    for cat in X_tracea[1].columns:\n",
    "        if type(X_tracea[1][cat][0]) == str:\n",
    "            cat_tas.append(cat)\n",
    "\n",
    "    uniques_cats = {}\n",
    "    for cat in cat_tas:\n",
    "        uniques_cats[cat] = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        for cat in cat_tas:\n",
    "            for reals in list(X_tracea[prefix][cat].unique()):\n",
    "                if not reals in uniques_cats[cat]:\n",
    "                    uniques_cats[cat].append(reals)\n",
    "\n",
    "    for cat in cat_tas:\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for j in list(X_tracea[prefix].index):\n",
    "                X_tracea[prefix][cat][j] = uniques_cats[cat].index(X_tracea[prefix][cat][j])\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "    models_collect = {}\n",
    "    dev_trained = []\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "    for d in dev:\n",
    "        time_start = time.clock()\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat = pd.get_dummies(imb_ref_enc_dat)\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_events[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_events[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_events[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_te_c = X_events[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_events[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_events[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "                [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "            pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_event = x_tr\n",
    "                X_test_event = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val_event = x_va\n",
    "                y_val = y_va\n",
    "                X_train_event_c = x_tr_c\n",
    "                X_test_event_c = x_te_c\n",
    "                X_val_event_c = x_va_c\n",
    "                y_train_c = y_tr_c\n",
    "                y_test_c = y_te_c\n",
    "                y_val_c = y_va_c\n",
    "            else:\n",
    "                X_train_event = np.append(X_train_event, x_tr, axis=0)\n",
    "                X_test_event = np.append(X_test_event, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val_event = np.append(X_val_event, x_va, axis=0)\n",
    "                X_train_event_c = np.append(X_train_event_c, x_tr_c, axis=0)\n",
    "                X_test_event_c = np.append(X_test_event_c, x_te_c, axis=0)\n",
    "                X_val_event_c = np.append(X_val_event_c, x_va_c, axis=0)\n",
    "                y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "                y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "                y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_event), len(y_train), len(y_train_c), len(X_val_event), len(y_val), len(y_val_c),\n",
    "              len(X_test_event), len(y_test), len(y_test_c))\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_resource[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_resource[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_resource[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_resource[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_resource[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_resource[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_resource = x_tr\n",
    "                X_test_resource = x_te\n",
    "                X_val_resource = x_va\n",
    "                X_train_resource_c = x_tr_c\n",
    "                X_test_resource_c = x_te_c\n",
    "                X_val_resource_c = x_va_c\n",
    "            else:\n",
    "                X_train_resource = np.append(X_train_resource, x_tr, axis=0)\n",
    "                X_test_resource = np.append(X_test_resource, x_te, axis=0)\n",
    "                X_val_resource = np.append(X_val_resource, x_va, axis=0)\n",
    "                X_train_resource_c = np.append(X_train_resource_c, x_tr_c, axis=0)\n",
    "                X_test_resource_c = np.append(X_test_resource_c, x_te_c, axis=0)\n",
    "                X_val_resource_c = np.append(X_val_resource_c, x_va_c,\n",
    "                                             axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_resource), len(y_train), len(X_val_resource), len(y_val), len(X_test_resource), len(y_test))\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_month[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_month[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_month[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_month[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_month[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_month[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_m = x_tr\n",
    "                X_test_m = x_te\n",
    "                X_val_m = x_va\n",
    "                X_train_m_c = x_tr_c\n",
    "                X_test_m_c = x_te_c\n",
    "                X_val_m_c = x_va_c\n",
    "            else:\n",
    "                X_train_m = np.append(X_train_m, x_tr, axis=0)\n",
    "                X_test_m = np.append(X_test_m, x_te, axis=0)\n",
    "                X_val_m = np.append(X_val_m, x_va, axis=0)\n",
    "                X_train_m_c = np.append(X_train_m_c, x_tr_c, axis=0)\n",
    "                X_test_m_c = np.append(X_test_m_c, x_te_c, axis=0)\n",
    "                X_val_m_c = np.append(X_val_m_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_m), len(y_train), len(X_val_m), len(y_val), len(X_test_m), len(y_test))\n",
    "\n",
    "        print('split done')\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_tracea[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_tracea[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_tracea[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_tracea[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_tracea[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_tracea[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_TA = x_tr\n",
    "                X_test_TA = x_te\n",
    "                X_val_TA = x_va\n",
    "                X_train_TA_c = x_tr_c\n",
    "                X_test_TA_c = x_te_c\n",
    "                X_val_TA_c = x_va_c\n",
    "            else:\n",
    "                X_train_TA = np.append(X_train_TA, x_tr, axis=0)\n",
    "                X_test_TA = np.append(X_test_TA, x_te, axis=0)\n",
    "                X_val_TA = np.append(X_val_TA, x_va, axis=0)\n",
    "                X_train_TA_c = np.append(X_train_TA_c, x_tr_c, axis=0)\n",
    "                X_test_TA_c = np.append(X_test_TA_c, x_te_c, axis=0)\n",
    "                X_val_TA_c = np.append(X_val_TA_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_TA), len(y_train), len(X_val_TA), len(y_val), len(X_test_TA), len(y_test))\n",
    "\n",
    "        events_encoder = list(\n",
    "            np.unique(np.append(np.append(X_train_event_c, X_test_event_c, axis=0), X_val_event_c, axis=0)))\n",
    "        events_encoder.index('No')\n",
    "        resource_encoder = list(\n",
    "            np.unique(np.append(np.append(X_train_resource_c, X_test_resource_c, axis=0), X_val_resource_c, axis=0)))\n",
    "        resource_encoder.index('No')\n",
    "        cat_ecnoders = {}\n",
    "        for cat in cat_tas:\n",
    "            cat_ecnoders[cat] = list(np.unique(np.append(np.append(X_train_TA_c, X_test_TA_c, axis=0), X_val_TA_c, axis=0)))\n",
    "\n",
    "        month_encoder = list(np.unique(np.append(np.append(X_train_m_c, X_test_m_c, axis=0), X_val_m_c, axis=0)))\n",
    "        month_encoder\n",
    "        for i in range(len(X_test_event)):\n",
    "            for j in range(len(X_test_event[0])):\n",
    "                X_test_event[i][j] = events_encoder.index(X_test_event[i][j])\n",
    "            for j in range(len(X_test_resource[0])):\n",
    "                X_test_resource[i][j] = resource_encoder.index(X_test_resource[i][j])\n",
    "            for j in range(len(X_test_m[0])):\n",
    "                X_test_m[i][j] = month_encoder.index(X_test_m[i][j])\n",
    "        for i in range(len(X_train_event)):\n",
    "            for j in range(len(X_train_event[0])):\n",
    "                X_train_event[i][j] = events_encoder.index(X_train_event[i][j])\n",
    "            for j in range(len(X_train_resource[0])):\n",
    "                X_train_resource[i][j] = resource_encoder.index(X_train_resource[i][j])\n",
    "            for j in range(len(X_train_m[0])):\n",
    "                X_train_m[i][j] = month_encoder.index(X_train_m[i][j])\n",
    "        for i in range(len(X_val_event)):\n",
    "            for j in range(len(X_val_event[0])):\n",
    "                X_val_event[i][j] = events_encoder.index(X_val_event[i][j])\n",
    "            for j in range(len(X_val_resource[0])):\n",
    "                X_val_resource[i][j] = resource_encoder.index(X_val_resource[i][j])\n",
    "            for j in range(len(X_val_m[0])):\n",
    "                X_val_m[i][j] = month_encoder.index(X_val_m[i][j])\n",
    "\n",
    "        # for combs_output\n",
    "        for i in range(len(X_test_event_c)):\n",
    "            for j in range(len(X_test_event_c[0])):\n",
    "                X_test_event_c[i][j] = events_encoder.index(X_test_event_c[i][j])\n",
    "            for j in range(len(X_test_resource[0])):\n",
    "                X_test_resource_c[i][j] = resource_encoder.index(X_test_resource_c[i][j])\n",
    "            for j in range(len(X_test_m[0])):\n",
    "                X_test_m_c[i][j] = month_encoder.index(X_test_m_c[i][j])\n",
    "        for i in range(len(X_train_event_c)):\n",
    "            for j in range(len(X_train_event_c[0])):\n",
    "                X_train_event_c[i][j] = events_encoder.index(X_train_event_c[i][j])\n",
    "            for j in range(len(X_train_resource[0])):\n",
    "                X_train_resource_c[i][j] = resource_encoder.index(X_train_resource_c[i][j])\n",
    "            for j in range(len(X_train_m[0])):\n",
    "                X_train_m_c[i][j] = month_encoder.index(X_train_m_c[i][j])\n",
    "        for i in range(len(X_val_event_c)):\n",
    "            for j in range(len(X_val_event_c[0])):\n",
    "                X_val_event_c[i][j] = events_encoder.index(X_val_event_c[i][j])\n",
    "            for j in range(len(X_val_resource_c[0])):\n",
    "                X_val_resource_c[i][j] = resource_encoder.index(X_val_resource_c[i][j])\n",
    "            for j in range(len(X_val_m_c[0])):\n",
    "                X_val_m_c[i][j] = month_encoder.index(X_val_m_c[i][j])\n",
    "        scaler = StandardScaler()\n",
    "        X_test_TA = scaler.fit_transform(X_test_TA)\n",
    "        X_train_TA = scaler.fit_transform(X_train_TA)\n",
    "        X_val_TA = scaler.fit_transform(X_val_TA)\n",
    "        X_test_TA_c = scaler.fit_transform(X_test_TA_c)\n",
    "        X_train_TA_c = scaler.fit_transform(X_train_TA_c)\n",
    "        X_val_TA_c = scaler.fit_transform(X_val_TA_c)\n",
    "        X_test_event = X_test_event.astype(int)\n",
    "        X_train_event = X_train_event.astype(int)\n",
    "        X_val_event = X_val_event.astype(int)\n",
    "        X_test_resource = X_test_resource.astype(int)\n",
    "        X_train_resource = X_train_resource.astype(int)\n",
    "        X_val_resource = X_val_resource.astype(int)\n",
    "        X_test_m = X_test_m.astype(int)\n",
    "        X_train_m = X_train_m.astype(int)\n",
    "        X_val_m = X_val_m.astype(int)\n",
    "        #for combs again\n",
    "        X_test_event_c = X_test_event_c.astype(int)\n",
    "        X_train_event_c = X_train_event_c.astype(int)\n",
    "        X_val_event_c = X_val_event_c.astype(int)\n",
    "        X_test_resource_c = X_test_resource_c.astype(int)\n",
    "        X_train_resource_c = X_train_resource_c.astype(int)\n",
    "        X_val_resource_c = X_val_resource_c.astype(int)\n",
    "        X_test_m_c = X_test_m_c.astype(int)\n",
    "        X_train_m_c = X_train_m_c.astype(int)\n",
    "        X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BPDP_LSTM(vocab_events=len(events_encoder), vocab_resources=len(resource_encoder), no_TA=len(X_train_TA[0]),\n",
    "                          vocab_month=len(month_encoder))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data_event = TrainData(torch.FloatTensor(X_train_event),\n",
    "                                         torch.FloatTensor(y_train))\n",
    "            train_data_resource = TestData(torch.FloatTensor(X_train_resource))\n",
    "            train_data_TA = TestData(torch.FloatTensor(X_train_TA))\n",
    "            train_data_m = TestData(torch.FloatTensor(X_train_m))\n",
    "            train_loader_event = DataLoader(dataset=train_data_event, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_resource = DataLoader(dataset=train_data_resource, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_TA = DataLoader(dataset=train_data_TA, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_m = DataLoader(dataset=train_data_m, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader_event))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                steps_r = list((train_loader_resource))\n",
    "                steps_ta = list((train_loader_TA))\n",
    "                steps_m = list((train_loader_m))\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                         steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(torch.FloatTensor(X_val_event).to(torch.int64),\n",
    "                                     torch.FloatTensor(X_val_resource).to(torch.int64),\n",
    "                                     torch.FloatTensor(X_val_TA).to(torch.float),\n",
    "                                     torch.FloatTensor(X_val_m).to(torch.int64))\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader_event)}, Acc: {epoch_acc / len(train_loader_event):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader_event):}, Acc: {epoch_acc / len(train_loader_event):.3f}\")\n",
    "\n",
    "        y_batch_pred\n",
    "        model.eval()\n",
    "        test_data_event = TestData(torch.FloatTensor(X_test_event))\n",
    "        test_data_resource = TestData(torch.FloatTensor(X_test_resource))\n",
    "        test_data_TA = TestData(torch.FloatTensor(X_test_TA))\n",
    "        test_data_m = TestData(torch.FloatTensor(X_test_m))\n",
    "        test_loader_event = DataLoader(dataset=test_data_event, batch_size=1)\n",
    "        test_loader_resource = DataLoader(dataset=test_data_resource, batch_size=1)\n",
    "        test_loader_TA = DataLoader(dataset=test_data_TA, batch_size=1)\n",
    "        test_loader_m = DataLoader(dataset=test_data_m, batch_size=1)\n",
    "        iterations_r = iter(test_loader_resource)\n",
    "        iterations_ta = iter(test_loader_TA)\n",
    "        iterations_m = iter(test_loader_m)\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, X_batch in enumerate(test_loader_event):\n",
    "                X_batch = X_batch.to(device).to(torch.int64)\n",
    "                y_test_pred = torch.nn.functional.softmax(\n",
    "                    model(X_batch, next(iterations_r).to(torch.int64), next(iterations_ta).to(torch.float),\n",
    "                          next(iterations_m).to(torch.int64)))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        cum_trace_idxs = flatten_comprehension(cum_trace_idxs)\n",
    "        pref_list = flatten_comprehension(pref_list)\n",
    "        if d == dev[0]:\n",
    "            y_test_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_test_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_test_cum['prefix_length'] = pref_list\n",
    "            y_pred_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_pred_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_pred_cum['prefix_length'] = pref_list\n",
    "        y_pred_cum[str('confidence' + d)] = y_confidence_list\n",
    "        y_test_cum[d] = list(y_test)\n",
    "        y_pred_cum[d] = y_pred_list\n",
    "\n",
    "        time_elapsed = time_start - time.clock()\n",
    "        metrics[d]['Time']=time_elapsed\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        print(metrics)\n",
    "        models_collect[d] = model\n",
    "\n",
    "        X_test_event_c = X_test_event_c.astype(int)\n",
    "        X_train_event_c = X_train_event_c.astype(int)\n",
    "        X_val_event_c = X_val_event_c.astype(int)\n",
    "        X_test_resource_c = X_test_resource_c.astype(int)\n",
    "        X_train_resource_c = X_train_resource_c.astype(int)\n",
    "        X_val_resource_c = X_val_resource_c.astype(int)\n",
    "        X_test_m_c = X_test_m_c.astype(int)\n",
    "        X_train_m_c = X_train_m_c.astype(int)\n",
    "        X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "        train_data_f_combs_event = TestData(torch.FloatTensor(X_train_event_c))\n",
    "        train_loader_f_combs_event = DataLoader(dataset=train_data_f_combs_event, batch_size=len(X_train_event_c))\n",
    "        train_data_f_combs_resource = TestData(torch.FloatTensor(X_train_resource_c))\n",
    "        train_loader_f_combs_resource = DataLoader(dataset=train_data_f_combs_resource, batch_size=len(X_train_resource_c))\n",
    "        train_data_f_combs_m = TestData(torch.FloatTensor(X_train_m_c))\n",
    "        train_loader_f_combs_m = DataLoader(dataset=train_data_f_combs_m, batch_size=len(X_train_m_c))\n",
    "        train_data_f_combs_TA = TestData(torch.FloatTensor(X_train_TA_c))\n",
    "        train_loader_f_combs_TA = DataLoader(dataset=train_data_f_combs_TA, batch_size=len(X_train_TA_c))\n",
    "\n",
    "        y_output_train = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((train_loader_f_combs_resource))\n",
    "            steps_ta = list((train_loader_f_combs_TA))\n",
    "            steps_m = list((train_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(train_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_train.append(y_test_pred.numpy())\n",
    "\n",
    "        test_data_f_combs_event = TestData(torch.FloatTensor(X_test_event_c))\n",
    "        test_loader_f_combs_event = DataLoader(dataset=test_data_f_combs_event, batch_size=len(X_test_event_c))\n",
    "        test_data_f_combs_resource = TestData(torch.FloatTensor(X_test_resource_c))\n",
    "        test_loader_f_combs_resource = DataLoader(dataset=test_data_f_combs_resource, batch_size=len(X_test_resource_c))\n",
    "        test_data_f_combs_m = TestData(torch.FloatTensor(X_test_m_c))\n",
    "        test_loader_f_combs_m = DataLoader(dataset=test_data_f_combs_m, batch_size=len(X_test_m_c))\n",
    "        test_data_f_combs_TA = TestData(torch.FloatTensor(X_test_TA_c))\n",
    "        test_loader_f_combs_TA = DataLoader(dataset=test_data_f_combs_TA, batch_size=len(X_test_TA_c))\n",
    "\n",
    "        y_output_test = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((test_loader_f_combs_resource))\n",
    "            steps_ta = list((test_loader_f_combs_TA))\n",
    "            steps_m = list((test_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(test_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_test.append(y_test_pred.numpy())\n",
    "\n",
    "        val_data_f_combs_event = TestData(torch.FloatTensor(X_val_event_c))\n",
    "        val_loader_f_combs_event = DataLoader(dataset=val_data_f_combs_event, batch_size=len(X_val_event_c))\n",
    "        val_data_f_combs_resource = TestData(torch.FloatTensor(X_val_resource_c))\n",
    "        val_loader_f_combs_resource = DataLoader(dataset=val_data_f_combs_resource, batch_size=len(X_val_resource_c))\n",
    "        val_data_f_combs_m = TestData(torch.FloatTensor(X_val_m_c))\n",
    "        val_loader_f_combs_m = DataLoader(dataset=val_data_f_combs_m, batch_size=len(X_val_m_c))\n",
    "        val_data_f_combs_TA = TestData(torch.FloatTensor(X_val_TA_c))\n",
    "        val_loader_f_combs_TA = DataLoader(dataset=val_data_f_combs_TA, batch_size=len(X_val_TA_c))\n",
    "\n",
    "        y_output_val = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((val_loader_f_combs_resource))\n",
    "            steps_ta = list((val_loader_f_combs_TA))\n",
    "            steps_m = list((val_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(val_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_val.append(y_test_pred.numpy())\n",
    "\n",
    "        outputs_train['NoDev' + str(d)] = y_output_train[0][:, 0]\n",
    "        outputs_train['Dev' + str(d)] = y_output_train[0][:, 1]\n",
    "        outputs_test['NoDev' + str(d)] = y_output_test[0][:, 0]\n",
    "        outputs_test['Dev' + str(d)] = y_output_test[0][:, 1]\n",
    "        outputs_val['NoDev' + str(d)] = y_output_val[0][:, 0]\n",
    "        outputs_val['Dev' + str(d)] = y_output_val[0][:, 1]\n",
    "        if d == dev[0]:\n",
    "            outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "            outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "            outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()\n",
    "\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    metrics_comb\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs(nn.Module):\n",
    "        def __init__(self, no_columns, no_devs):\n",
    "            super(Ensemble_Stack_Combs, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 64)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(128, 64)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(64, no_devs)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(64)\n",
    "            self.batchnorm2 = nn.LayerNorm(64)\n",
    "            self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.layer_1(inputs)\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            #x = self.layer_2(x)\n",
    "            #x = self.activation2(self.layer_2(x))\n",
    "            #x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            #x = self.Sigmoid(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs_Single(nn.Module):\n",
    "        def __init__(self, no_columns):\n",
    "            super(Ensemble_Stack_Combs_Single, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 512)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(512, 128)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(128, 2)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(512)\n",
    "            self.batchnorm2 = nn.LayerNorm(128)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.activation2(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            return x\n",
    "\n",
    "    len(y_train_c)\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for i, label in enumerate(unique_rel_combs):\n",
    "        positives[label] = y_train_c[:, i].sum()\n",
    "        negatives[label] = len(y_train_c) - y_train_c[:, i].sum()\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pir[label] = min((max(positives[label], negatives[label]) / positives[label]), 10000)\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    pw_combs = {}\n",
    "    nw_combs = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pw_combs[label] = min((mean(pir.values()) ** (4 / (2 ** math.e)) + (np.log(pirlbl[label]))), 200)\n",
    "        #pw_combs[label] =  min(2*(mean(pir.values()) ** ((2* math.e)) + (np.log(pirlbl[label]))), 100000)\n",
    "        nw_combs[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    pw_combs\n",
    "    if len(y_train_c[0]) == 1:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "\n",
    "        model = Ensemble_Stack_Combs_Single(no_columns=len(outputs_train_us[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor([8, 1])\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            y_train_c_us['NoDev']=0\n",
    "            for je in list(y_train_c_us.index):\n",
    "                y_train_c_us['NoDev'][je]=1-y_train_c_us[i][je]\n",
    "            y_val = np.array(y_val_c[:, i], )\n",
    "            y_val = np.column_stack((y_val, 1 - y_val))\n",
    "            y_test = np.array(y_test_c[:, i], )\n",
    "            y_test = np.column_stack((y_test, 1 - y_test))\n",
    "\n",
    "            if early_stop:\n",
    "                EPOCHS = 300\n",
    "                model.train()\n",
    "                train_data = TrainData(torch.FloatTensor(outputs_train_us),\n",
    "                                       torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "\n",
    "                train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "                X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "                es = EarlyStopping()\n",
    "                done = False\n",
    "\n",
    "                epoch = 0\n",
    "                while epoch < EPOCHS and not done:\n",
    "                    epoch += 1\n",
    "                    steps = list(enumerate(train_loader))\n",
    "                    pbar = tqdm.tqdm(steps)\n",
    "                    model.train()\n",
    "                    epoch_acc = 0\n",
    "                    epoch_loss = 0\n",
    "                    for i, (x_batch, y_batch) in pbar:\n",
    "                        optimizer.zero_grad()\n",
    "                        y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                        loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                        acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_acc += acc.item()\n",
    "\n",
    "                        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                        epoch_loss += loss\n",
    "                        if i == len(steps) - 1:\n",
    "                            model.eval()\n",
    "                            pred = model(X_val)\n",
    "                            vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                            if es(model, vloss): done = True\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                        else:\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "            model.eval()\n",
    "            test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "            test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "            y_pred_list = []\n",
    "            y_confidence_list = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch in test_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                    y_confidence_list.append(y_test_pred.numpy())\n",
    "                    y_pred_tag = torch.round(y_test_pred)\n",
    "                    y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "            y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "            print(CM)\n",
    "\n",
    "            metrics_comb[urc]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics_comb[urc]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "    else:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "        model = Ensemble_Stack_Combs(no_columns=len(outputs_train_us[0]),\n",
    "                                     no_devs=len(y_train_c_us.loc[list(y_train_c_us.index)[0]]))\n",
    "        #model = Ensemble_Stack_Combs(no_columns=len(outputs_train.loc[0]), no_devs=len(y_train_c[0]))\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(list(pw_combs.values())))\n",
    "        #criterion = nn.MultiLabelSoftMarginLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(outputs_train_us), torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "            #train_data = TrainData(torch.FloatTensor(outputs_train.to_numpy()),torch.FloatTensor(y_train_c))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val_c))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.sigmoid(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "        print(CM)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "            metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                             average='macro')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    metrics_comb\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_BPDP_stacked_CIBE_LSTM.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_LSTM_DPP_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,relevance_ths = .5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_collective_LSTM_DPP_CIBE(log, ref_log, aligned_traces, u_sample = True,early_stop = True,explained = False,split = 1 / 3,relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "\n",
    "    ref_enc_dat = ref_clean_dat.copy()\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.00001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = clean_dat.copy()\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 'No'  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='No')\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_LSTM')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    ref_enc_dat\n",
    "\n",
    "    evs_c = []\n",
    "    resource_c = []\n",
    "    month_c = []\n",
    "    trace_attr = []\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('event'): evs_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('resource'): resource_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('month'): month_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if not (ca in evs_c or ca in resource_c or ca in month_c): trace_attr.append(ca)\n",
    "    print(evs_c)\n",
    "    print(resource_c)\n",
    "    print(month_c)\n",
    "    print(trace_attr)\n",
    "\n",
    "    X_events = {}\n",
    "    X_resource = {}\n",
    "    X_month = {}\n",
    "    X_tracea = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        X_events[prefix] = X_cum[prefix][evs_c]\n",
    "        X_resource[prefix] = X_cum[prefix][resource_c]\n",
    "        X_month[prefix] = X_cum[prefix][month_c]\n",
    "        X_tracea[prefix] = X_cum[prefix][trace_attr]\n",
    "\n",
    "    cat_tas = []\n",
    "    for cat in X_tracea[1].columns:\n",
    "        if type(X_tracea[1][cat][0]) == str:\n",
    "            cat_tas.append(cat)\n",
    "    cat_tas\n",
    "    uniques_cats = {}\n",
    "    for cat in cat_tas:\n",
    "        uniques_cats[cat] = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        for cat in cat_tas:\n",
    "            for reals in list(X_tracea[prefix][cat].unique()):\n",
    "                if not reals in uniques_cats[cat]:\n",
    "                    uniques_cats[cat].append(reals)\n",
    "    uniques_cats\n",
    "    X_tracea[1]\n",
    "    for cat in cat_tas:\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for j in list(X_tracea[prefix].index):\n",
    "                X_tracea[prefix][cat][j] = uniques_cats[cat].index(X_tracea[prefix][cat][j])\n",
    "    X_tracea[1]\n",
    "    X_events[1]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 8\n",
    "        negative_weights[label] = 1\n",
    "    models_collect = {}\n",
    "    dev_trained = []\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "    Y_cum_dev = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "        Y_cum_dev[prefix]['NoDev'] = 0\n",
    "        for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "            Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "        if prefix == 1:\n",
    "            print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "    if u_sample:\n",
    "        imb_ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "        imb_ref_enc_dat['ind'] = 0\n",
    "        for i in range(len(imb_ref_enc_dat)):\n",
    "            imb_ref_enc_dat['ind'][i] = i\n",
    "\n",
    "        imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "        for trace in range(len(log)):\n",
    "            if dev_df.loc[trace].sum() > 0:\n",
    "                imb_traces['Dev'][trace] = 1\n",
    "\n",
    "        imb_traces = imb_traces.drop(x_test_idx)\n",
    "        imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "        x_train_idx = list(X_resampled['ind'])\n",
    "        y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "    print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "    # validation set for early stopping\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "\n",
    "                                                                      random_state=0)\n",
    "\n",
    "    enumerated_trace_idx = {}\n",
    "    cum_trace_idxs = []\n",
    "    pref_list = []\n",
    "    pref_list_train_c = []\n",
    "    pref_list_test_c = []\n",
    "    pref_list_val_c = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_events[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_events[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_events[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        y_te = y_cum_test[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_tr = y_cum_test[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va = y_cum_test[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_te_c = X_events[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_events[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_events[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "            float)\n",
    "        y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "            [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "            float)\n",
    "        cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "        pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "        enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "        pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "        pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "        pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_event = x_tr\n",
    "            X_test_event = x_te\n",
    "            y_train = y_tr\n",
    "            y_test = y_te\n",
    "            X_val_event = x_va\n",
    "            y_val = y_va\n",
    "            X_train_event_c = x_tr_c\n",
    "            X_test_event_c = x_te_c\n",
    "            X_val_event_c = x_va_c\n",
    "            y_train_c = y_tr_c\n",
    "            y_test_c = y_te_c\n",
    "            y_val_c = y_va_c\n",
    "        else:\n",
    "            X_train_event = np.append(X_train_event, x_tr, axis=0)\n",
    "            X_test_event = np.append(X_test_event, x_te, axis=0)\n",
    "            y_train = np.append(y_train, y_tr, axis=0)\n",
    "            y_test = np.append(y_test, y_te, axis=0)\n",
    "            y_val = np.append(y_val, y_va, axis=0)\n",
    "            X_val_event = np.append(X_val_event, x_va, axis=0)\n",
    "            X_train_event_c = np.append(X_train_event_c, x_tr_c, axis=0)\n",
    "            X_test_event_c = np.append(X_test_event_c, x_te_c, axis=0)\n",
    "            X_val_event_c = np.append(X_val_event_c, x_va_c, axis=0)\n",
    "            y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "            y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "            y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_event), len(y_train), len(y_train_c), len(X_val_event), len(y_val), len(y_val_c),\n",
    "          len(X_test_event), len(y_test), len(y_test_c))\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_resource[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_resource[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_resource[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_resource[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_resource[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_resource[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_resource = x_tr\n",
    "            X_test_resource = x_te\n",
    "            X_val_resource = x_va\n",
    "            X_train_resource_c = x_tr_c\n",
    "            X_test_resource_c = x_te_c\n",
    "            X_val_resource_c = x_va_c\n",
    "        else:\n",
    "            X_train_resource = np.append(X_train_resource, x_tr, axis=0)\n",
    "            X_test_resource = np.append(X_test_resource, x_te, axis=0)\n",
    "            X_val_resource = np.append(X_val_resource, x_va, axis=0)\n",
    "            X_train_resource_c = np.append(X_train_resource_c, x_tr_c, axis=0)\n",
    "            X_test_resource_c = np.append(X_test_resource_c, x_te_c, axis=0)\n",
    "            X_val_resource_c = np.append(X_val_resource_c, x_va_c,\n",
    "                                         axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_resource), len(y_train), len(X_val_resource), len(y_val), len(X_test_resource), len(y_test))\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_month[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_month[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_month[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_month[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_month[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_month[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_m = x_tr\n",
    "            X_test_m = x_te\n",
    "            X_val_m = x_va\n",
    "            X_train_m_c = x_tr_c\n",
    "            X_test_m_c = x_te_c\n",
    "            X_val_m_c = x_va_c\n",
    "        else:\n",
    "            X_train_m = np.append(X_train_m, x_tr, axis=0)\n",
    "            X_test_m = np.append(X_test_m, x_te, axis=0)\n",
    "            X_val_m = np.append(X_val_m, x_va, axis=0)\n",
    "            X_train_m_c = np.append(X_train_m_c, x_tr_c, axis=0)\n",
    "            X_test_m_c = np.append(X_test_m_c, x_te_c, axis=0)\n",
    "            X_val_m_c = np.append(X_val_m_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_m), len(y_train), len(X_val_m), len(y_val), len(X_test_m), len(y_test))\n",
    "\n",
    "    print('split done')\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_tracea[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_tracea[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_tracea[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_tracea[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_tracea[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_tracea[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_TA = x_tr\n",
    "            X_test_TA = x_te\n",
    "            X_val_TA = x_va\n",
    "            X_train_TA_c = x_tr_c\n",
    "            X_test_TA_c = x_te_c\n",
    "            X_val_TA_c = x_va_c\n",
    "        else:\n",
    "            X_train_TA = np.append(X_train_TA, x_tr, axis=0)\n",
    "            X_test_TA = np.append(X_test_TA, x_te, axis=0)\n",
    "            X_val_TA = np.append(X_val_TA, x_va, axis=0)\n",
    "            X_train_TA_c = np.append(X_train_TA_c, x_tr_c, axis=0)\n",
    "            X_test_TA_c = np.append(X_test_TA_c, x_te_c, axis=0)\n",
    "            X_val_TA_c = np.append(X_val_TA_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_TA), len(y_train), len(X_val_TA), len(y_val), len(X_test_TA), len(y_test))\n",
    "\n",
    "    events_encoder = list(\n",
    "        np.unique(np.append(np.append(X_train_event_c, X_test_event_c, axis=0), X_val_event_c, axis=0)))\n",
    "    events_encoder.index('No')\n",
    "    resource_encoder = list(\n",
    "        np.unique(np.append(np.append(X_train_resource_c, X_test_resource_c, axis=0), X_val_resource_c, axis=0)))\n",
    "    resource_encoder.index('No')\n",
    "    cat_ecnoders = {}\n",
    "    for cat in cat_tas:\n",
    "        cat_ecnoders[cat] = list(np.unique(np.append(np.append(X_train_TA_c, X_test_TA_c, axis=0), X_val_TA_c, axis=0)))\n",
    "\n",
    "    month_encoder = list(np.unique(np.append(np.append(X_train_m_c, X_test_m_c, axis=0), X_val_m_c, axis=0)))\n",
    "    month_encoder\n",
    "    for i in range(len(X_test_event)):\n",
    "        for j in range(len(X_test_event[0])):\n",
    "            X_test_event[i][j] = events_encoder.index(X_test_event[i][j])\n",
    "        for j in range(len(X_test_resource[0])):\n",
    "            X_test_resource[i][j] = resource_encoder.index(X_test_resource[i][j])\n",
    "        for j in range(len(X_test_m[0])):\n",
    "            X_test_m[i][j] = month_encoder.index(X_test_m[i][j])\n",
    "    for i in range(len(X_train_event)):\n",
    "        for j in range(len(X_train_event[0])):\n",
    "            X_train_event[i][j] = events_encoder.index(X_train_event[i][j])\n",
    "        for j in range(len(X_train_resource[0])):\n",
    "            X_train_resource[i][j] = resource_encoder.index(X_train_resource[i][j])\n",
    "        for j in range(len(X_train_m[0])):\n",
    "            X_train_m[i][j] = month_encoder.index(X_train_m[i][j])\n",
    "    for i in range(len(X_val_event)):\n",
    "        for j in range(len(X_val_event[0])):\n",
    "            X_val_event[i][j] = events_encoder.index(X_val_event[i][j])\n",
    "        for j in range(len(X_val_resource[0])):\n",
    "            X_val_resource[i][j] = resource_encoder.index(X_val_resource[i][j])\n",
    "        for j in range(len(X_val_m[0])):\n",
    "            X_val_m[i][j] = month_encoder.index(X_val_m[i][j])\n",
    "\n",
    "    # for combs_output\n",
    "    for i in range(len(X_test_event_c)):\n",
    "        for j in range(len(X_test_event_c[0])):\n",
    "            X_test_event_c[i][j] = events_encoder.index(X_test_event_c[i][j])\n",
    "        for j in range(len(X_test_resource[0])):\n",
    "            X_test_resource_c[i][j] = resource_encoder.index(X_test_resource_c[i][j])\n",
    "        for j in range(len(X_test_m[0])):\n",
    "            X_test_m_c[i][j] = month_encoder.index(X_test_m_c[i][j])\n",
    "    for i in range(len(X_train_event_c)):\n",
    "        for j in range(len(X_train_event_c[0])):\n",
    "            X_train_event_c[i][j] = events_encoder.index(X_train_event_c[i][j])\n",
    "        for j in range(len(X_train_resource[0])):\n",
    "            X_train_resource_c[i][j] = resource_encoder.index(X_train_resource_c[i][j])\n",
    "        for j in range(len(X_train_m[0])):\n",
    "            X_train_m_c[i][j] = month_encoder.index(X_train_m_c[i][j])\n",
    "    for i in range(len(X_val_event_c)):\n",
    "        for j in range(len(X_val_event_c[0])):\n",
    "            X_val_event_c[i][j] = events_encoder.index(X_val_event_c[i][j])\n",
    "        for j in range(len(X_val_resource_c[0])):\n",
    "            X_val_resource_c[i][j] = resource_encoder.index(X_val_resource_c[i][j])\n",
    "        for j in range(len(X_val_m_c[0])):\n",
    "            X_val_m_c[i][j] = month_encoder.index(X_val_m_c[i][j])\n",
    "    scaler = StandardScaler()\n",
    "    X_test_TA = scaler.fit_transform(X_test_TA)\n",
    "    X_train_TA = scaler.fit_transform(X_train_TA)\n",
    "    X_val_TA = scaler.fit_transform(X_val_TA)\n",
    "    X_test_TA_c = scaler.fit_transform(X_test_TA_c)\n",
    "    X_train_TA_c = scaler.fit_transform(X_train_TA_c)\n",
    "    X_val_TA_c = scaler.fit_transform(X_val_TA_c)\n",
    "    X_test_event = X_test_event.astype(int)\n",
    "    X_train_event = X_train_event.astype(int)\n",
    "    X_val_event = X_val_event.astype(int)\n",
    "    X_test_resource = X_test_resource.astype(int)\n",
    "    X_train_resource = X_train_resource.astype(int)\n",
    "    X_val_resource = X_val_resource.astype(int)\n",
    "    X_test_m = X_test_m.astype(int)\n",
    "    X_train_m = X_train_m.astype(int)\n",
    "    X_val_m = X_val_m.astype(int)\n",
    "    #for combs again\n",
    "    X_test_event_c = X_test_event_c.astype(int)\n",
    "    X_train_event_c = X_train_event_c.astype(int)\n",
    "    X_val_event_c = X_val_event_c.astype(int)\n",
    "    X_test_resource_c = X_test_resource_c.astype(int)\n",
    "    X_train_resource_c = X_train_resource_c.astype(int)\n",
    "    X_val_resource_c = X_val_resource_c.astype(int)\n",
    "    X_test_m_c = X_test_m_c.astype(int)\n",
    "    X_train_m_c = X_train_m_c.astype(int)\n",
    "    X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "    y_train\n",
    "\n",
    "\n",
    "    class BPDP_LSTM_SC(nn.Module):\n",
    "        def __init__(self, vocab_events, vocab_resources, no_TA, vocab_month, no_devs):\n",
    "            super(BPDP_LSTM_SC, self).__init__()\n",
    "            self.embedding_e = nn.Embedding(vocab_events, 16)  # hier auf 8 / 16\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.lstm_e = nn.LSTM(input_size=16, hidden_size=128, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_e = nn.Linear(128, 32)\n",
    "            self.embedding_r = nn.Embedding(vocab_resources, 16)\n",
    "            self.lstm_r = nn.LSTM(input_size=16, hidden_size=128, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_r = nn.Linear(128, 32)\n",
    "            self.embedding_m = nn.Embedding(vocab_month, 16)\n",
    "            self.lstm_m = nn.LSTM(input_size=16, hidden_size=128, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_m = nn.Linear(128, 32)\n",
    "            self.linear_ta = nn.Linear(no_TA, 32)\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(128)\n",
    "            self.linear = nn.Linear(128, no_devs)\n",
    "\n",
    "        def forward(self, evs, rs, tas, ms):\n",
    "            evs = self.embedding_e(evs)\n",
    "            evs, _ = self.lstm_e(evs)\n",
    "            evs = self.linear_e(evs)\n",
    "            evs = evs[:, -1, :]\n",
    "            evs = self.activation1(evs)\n",
    "            rs = self.embedding_r(rs)\n",
    "            rs, _ = self.lstm_r(rs)\n",
    "            rs = rs[:, -1, :]\n",
    "            rs = self.activation1(rs)\n",
    "            rs = self.linear_r(rs)\n",
    "            ms = self.embedding_m(ms)\n",
    "            ms, _ = self.lstm_m(ms)\n",
    "            ms = ms[:, -1, :]\n",
    "            ms = self.activation1(ms)\n",
    "            ms = self.linear_m(ms)\n",
    "            tas = self.linear_ta(tas)\n",
    "            fin = torch.cat((evs, rs), dim=1)\n",
    "            fin = torch.cat((fin, ms), dim=1)\n",
    "            fin = torch.cat((fin, tas), dim=1)\n",
    "            fin = self.batchnorm1(fin)\n",
    "            #fin = self.dropout(fin)\n",
    "            fin = self.linear(fin)\n",
    "            return fin\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for label in labels:\n",
    "        positives[label] = sum(dev_df[label] == 1)\n",
    "        negatives[label] = sum(dev_df[label] == 0)\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in labels:\n",
    "        pir[label] = max(positives[label], negatives[label]) / positives[label]\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = mean(pir.values()) ** (1 / (2 * math.e)) + np.log(pirlbl[label])\n",
    "        negative_weights[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    positive_weights\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BPDP_LSTM_SC(vocab_events=len(events_encoder), vocab_resources=len(resource_encoder), no_TA=len(X_train_TA[0]),\n",
    "                         vocab_month=len(month_encoder), no_devs=len(y_train[0]))\n",
    "    model.to(device)\n",
    "    weights = torch.FloatTensor(list(positive_weights.values()))\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    if early_stop:\n",
    "        EPOCHS = 100\n",
    "        model.train()\n",
    "        train_data_event = TrainData(torch.FloatTensor(X_train_event),\n",
    "                                     torch.FloatTensor(y_train))\n",
    "        train_data_resource = TestData(torch.FloatTensor(X_train_resource))\n",
    "        train_data_TA = TestData(torch.FloatTensor(X_train_TA))\n",
    "        train_data_m = TestData(torch.FloatTensor(X_train_m))\n",
    "        train_loader_event = DataLoader(dataset=train_data_event, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_resource = DataLoader(dataset=train_data_resource, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_TA = DataLoader(dataset=train_data_TA, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_m = DataLoader(dataset=train_data_m, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        es = EarlyStopping()\n",
    "        done = False\n",
    "\n",
    "        epoch = 0\n",
    "        while epoch < EPOCHS and not done:\n",
    "            epoch += 1\n",
    "            steps = list(enumerate(train_loader_event))\n",
    "            pbar = tqdm.tqdm(steps)\n",
    "            steps_r = list((train_loader_resource))\n",
    "            steps_ta = list((train_loader_TA))\n",
    "            steps_m = list((train_loader_m))\n",
    "            model.train()\n",
    "            epoch_acc = 0\n",
    "            epoch_loss = 0\n",
    "            for i, (x_batch, y_batch) in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                y_batch_pred = model(x_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                     steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "\n",
    "                loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "                loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                epoch_loss += loss\n",
    "                if i == len(steps) - 1:\n",
    "                    model.eval()\n",
    "                    pred = model(torch.FloatTensor(X_val_event).to(torch.int64),\n",
    "                                 torch.FloatTensor(X_val_resource).to(torch.int64),\n",
    "                                 torch.FloatTensor(X_val_TA).to(torch.float),\n",
    "                                 torch.FloatTensor(X_val_m).to(torch.int64))\n",
    "                    vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                    if es(model, vloss): done = True\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader_event)}, Acc: {epoch_acc / len(train_loader_event):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                else:\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader_event):}, Acc: {epoch_acc / len(train_loader_event):.3f}\")\n",
    "\n",
    "    y_batch_pred\n",
    "    model.eval()\n",
    "    test_data_event = TestData(torch.FloatTensor(X_test_event))\n",
    "    test_data_resource = TestData(torch.FloatTensor(X_test_resource))\n",
    "    test_data_TA = TestData(torch.FloatTensor(X_test_TA))\n",
    "    test_data_m = TestData(torch.FloatTensor(X_test_m))\n",
    "    test_loader_event = DataLoader(dataset=test_data_event, batch_size=1)\n",
    "    test_loader_resource = DataLoader(dataset=test_data_resource, batch_size=1)\n",
    "    test_loader_TA = DataLoader(dataset=test_data_TA, batch_size=1)\n",
    "    test_loader_m = DataLoader(dataset=test_data_m, batch_size=1)\n",
    "    iterations_r = iter(test_loader_resource)\n",
    "    iterations_ta = iter(test_loader_TA)\n",
    "    iterations_m = iter(test_loader_m)\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, X_batch in enumerate(test_loader_event):\n",
    "            X_batch = X_batch.to(device).to(torch.int64)\n",
    "            y_test_pred = torch.nn.functional.sigmoid(\n",
    "                model(X_batch, next(iterations_r).to(torch.int64), next(iterations_ta).to(torch.float),\n",
    "                      next(iterations_m).to(torch.int64)))\n",
    "            y_pred_tag = torch.round(y_test_pred)\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    for i, d in enumerate(dev):\n",
    "        metrics[d]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[d]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        metrics[d]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                  average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    print(CM)\n",
    "\n",
    "    print(metrics)\n",
    "    writer = pd.ExcelWriter('BPDP_LSTM/' + z + '_BPDP_LSTM_SC_1.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()\n",
    "    cum_trace_idxs = flatten_comprehension(cum_trace_idxs)\n",
    "    pref_list = flatten_comprehension(pref_list)\n",
    "    y_test_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "    y_test_cum['trace_idx'] = cum_trace_idxs\n",
    "    y_test_cum['prefix_length'] = pref_list\n",
    "    y_pred_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "    y_pred_cum['trace_idx'] = cum_trace_idxs\n",
    "    y_pred_cum['prefix_length'] = pref_list\n",
    "\n",
    "    y_test[:, 2]\n",
    "    for i in range(len(dev)):\n",
    "        y_test_cum[dev[i]] = list(y_test[:, i])\n",
    "        y_pred_cum[dev[i]] = list(np.array(y_pred_list)[:, i])\n",
    "    print(metrics)\n",
    "\n",
    "    y_test_cum\n",
    "\n",
    "    train_data_event_f_combs = TestData(torch.FloatTensor(X_train_event_c))\n",
    "    train_loader_event_f_combs = DataLoader(dataset=train_data_event_f_combs, batch_size=len(X_train_event_c))\n",
    "    train_data_resource_f_combs = TestData(torch.FloatTensor(X_train_resource_c))\n",
    "    train_loader_resource_f_combs = DataLoader(dataset=train_data_resource_f_combs, batch_size=len(X_train_resource_c))\n",
    "    train_data_m_f_combs = TestData(torch.FloatTensor(X_train_m_c))\n",
    "    train_loader_m_f_combs = DataLoader(dataset=train_data_m_f_combs, batch_size=len(X_train_m_c))\n",
    "    train_data_TA_f_combs = TestData(torch.FloatTensor(X_train_TA_c))\n",
    "    train_loader_TA_f_combs = DataLoader(dataset=train_data_TA_f_combs, batch_size=len(X_train_TA_c))\n",
    "    steps_r = list((train_loader_resource_f_combs))\n",
    "    steps_ta = list((train_loader_TA_f_combs))\n",
    "    steps_m = list((train_loader_m_f_combs))\n",
    "\n",
    "    y_output_train = []\n",
    "    with torch.no_grad():\n",
    "        for i, X_batch in enumerate(train_loader_event_f_combs):\n",
    "            X_batch = X_batch.to(torch.long).to(device)\n",
    "            y_test_pred = torch.nn.functional.softmax(model(X_batch, steps_r[i].to(torch.int64).to(device),\n",
    "                                                            steps_ta[i].to(torch.float).to(device),\n",
    "                                                            steps_m[i].to(torch.int64).to(device)))\n",
    "            y_output_train.append(y_test_pred.numpy())\n",
    "\n",
    "    test_data_event_f_combs = TestData(torch.FloatTensor(X_test_event_c))\n",
    "    test_loader_event_f_combs = DataLoader(dataset=test_data_event_f_combs, batch_size=len(X_test_event_c))\n",
    "    test_data_resource_f_combs = TestData(torch.FloatTensor(X_test_resource_c))\n",
    "    test_loader_resource_f_combs = DataLoader(dataset=test_data_resource_f_combs, batch_size=len(X_test_resource_c))\n",
    "    test_data_m_f_combs = TestData(torch.FloatTensor(X_test_m_c))\n",
    "    test_loader_m_f_combs = DataLoader(dataset=test_data_m_f_combs, batch_size=len(X_test_m_c))\n",
    "    test_data_TA_f_combs = TestData(torch.FloatTensor(X_test_TA_c))\n",
    "    test_loader_TA_f_combs = DataLoader(dataset=test_data_TA_f_combs, batch_size=len(X_test_TA_c))\n",
    "    steps_r = list((test_loader_resource_f_combs))\n",
    "    steps_ta = list((test_loader_TA_f_combs))\n",
    "    steps_m = list((test_loader_m_f_combs))\n",
    "\n",
    "    y_output_test = []\n",
    "    with torch.no_grad():\n",
    "        for i, X_batch in enumerate(test_loader_event_f_combs):\n",
    "            X_batch = X_batch.to(torch.long).to(device)\n",
    "            y_test_pred = torch.nn.functional.softmax(model(X_batch, steps_r[i].to(torch.int64).to(device),\n",
    "                                                            steps_ta[i].to(torch.float).to(device),\n",
    "                                                            steps_m[i].to(torch.int64).to(device)))\n",
    "            y_output_test.append(y_test_pred.numpy())\n",
    "\n",
    "    val_data_event_f_combs = TestData(torch.FloatTensor(X_val_event_c))\n",
    "    val_loader_event_f_combs = DataLoader(dataset=val_data_event_f_combs, batch_size=len(X_val_event_c))\n",
    "    val_data_resource_f_combs = TestData(torch.FloatTensor(X_val_resource_c))\n",
    "    val_loader_resource_f_combs = DataLoader(dataset=val_data_resource_f_combs, batch_size=len(X_val_resource_c))\n",
    "    val_data_m_f_combs = TestData(torch.FloatTensor(X_val_m_c))\n",
    "    val_loader_m_f_combs = DataLoader(dataset=val_data_m_f_combs, batch_size=len(X_val_m_c))\n",
    "    val_data_TA_f_combs = TestData(torch.FloatTensor(X_val_TA_c))\n",
    "    val_loader_TA_f_combs = DataLoader(dataset=val_data_TA_f_combs, batch_size=len(X_val_TA_c))\n",
    "    steps_r = list((val_loader_resource_f_combs))\n",
    "    steps_ta = list((val_loader_TA_f_combs))\n",
    "    steps_m = list((val_loader_m_f_combs))\n",
    "\n",
    "    y_output_val = []\n",
    "    with torch.no_grad():\n",
    "        for i, X_batch in enumerate(val_loader_event_f_combs):\n",
    "            X_batch = X_batch.to(torch.long).to(device)\n",
    "            y_test_pred = torch.nn.functional.softmax(model(X_batch, steps_r[i].to(torch.int64).to(device),\n",
    "                                                            steps_ta[i].to(torch.float).to(device),\n",
    "                                                            steps_m[i].to(torch.int64).to(device)))\n",
    "            y_output_val.append(y_test_pred.numpy())\n",
    "\n",
    "    y_output_val[0][:, 0]\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "    for i in range(len(dev)):\n",
    "        outputs_train['Dev' + str(dev[i])] = y_output_train[0][:, i]\n",
    "        outputs_train['NoDev' + str(dev[i])] = 1 - outputs_train['Dev' + str(dev[i])]\n",
    "        outputs_test['Dev' + str(dev[i])] = y_output_test[0][:, i]\n",
    "        outputs_test['NoDev' + str(dev[i])] = 1 - outputs_test['Dev' + str(dev[i])]\n",
    "        outputs_val['Dev' + str(dev[i])] = y_output_val[0][:, i]\n",
    "        outputs_val['NoDev' + str(dev[i])] = 1 - outputs_val['Dev' + str(dev[i])]\n",
    "        outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "        outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "        outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "    outputs_test\n",
    "\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    metrics_comb\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs(nn.Module):\n",
    "        def __init__(self, no_columns, no_devs):\n",
    "            super(Ensemble_Stack_Combs, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 64)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(128, 64)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(64, no_devs)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(64)\n",
    "            self.batchnorm2 = nn.LayerNorm(64)\n",
    "            self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.layer_1(inputs)\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            #x = self.layer_2(x)\n",
    "            #x = self.activation2(self.layer_2(x))\n",
    "            #x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            #x = self.Sigmoid(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs_Single(nn.Module):\n",
    "        def __init__(self, no_columns):\n",
    "            super(Ensemble_Stack_Combs_Single, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 512)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(512, 128)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(128, 2)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(512)\n",
    "            self.batchnorm2 = nn.LayerNorm(128)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.activation2(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    len(y_train_c)\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for i, label in enumerate(unique_rel_combs):\n",
    "        positives[label] = y_train_c[:, i].sum()\n",
    "        negatives[label] = len(y_train_c) - y_train_c[:, i].sum()\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pir[label] = min((max(positives[label], negatives[label]) / positives[label]), 10000)\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    pw_combs = {}\n",
    "    nw_combs = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pw_combs[label] = min((mean(pir.values()) ** (4 / (2 ** math.e)) + (np.log(pirlbl[label]))), 200)\n",
    "        #pw_combs[label] =  min(2*(mean(pir.values()) ** ((2* math.e)) + (np.log(pirlbl[label]))), 100000)\n",
    "        nw_combs[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    pw_combs\n",
    "\n",
    "    if len(y_train_c[0]) == 1:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "\n",
    "        model = Ensemble_Stack_Combs_Single(no_columns=len(outputs_train_us[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor([8, 1])\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            y_train_c_us['NoDev'] = 0\n",
    "            for je in list(y_train_c_us.index):\n",
    "                y_train_c_us['NoDev'][je] = 1 - y_train_c_us[i][je]\n",
    "            y_val = np.array(y_val_c[:, i], )\n",
    "            y_val = np.column_stack((y_val, 1 - y_val))\n",
    "            y_test = np.array(y_test_c[:, i], )\n",
    "            y_test = np.column_stack((y_test, 1 - y_test))\n",
    "\n",
    "            if early_stop:\n",
    "                EPOCHS = 300\n",
    "                model.train()\n",
    "                train_data = TrainData(torch.FloatTensor(outputs_train_us),\n",
    "                                       torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "\n",
    "                train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "                X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "                es = EarlyStopping()\n",
    "                done = False\n",
    "\n",
    "                epoch = 0\n",
    "                while epoch < EPOCHS and not done:\n",
    "                    epoch += 1\n",
    "                    steps = list(enumerate(train_loader))\n",
    "                    pbar = tqdm.tqdm(steps)\n",
    "                    model.train()\n",
    "                    epoch_acc = 0\n",
    "                    epoch_loss = 0\n",
    "                    for i, (x_batch, y_batch) in pbar:\n",
    "                        optimizer.zero_grad()\n",
    "                        y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                        loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                        acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_acc += acc.item()\n",
    "\n",
    "                        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                        epoch_loss += loss\n",
    "                        if i == len(steps) - 1:\n",
    "                            model.eval()\n",
    "                            pred = model(X_val)\n",
    "                            vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                            if es(model, vloss): done = True\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                        else:\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "            model.eval()\n",
    "            test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "            test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "            y_pred_list = []\n",
    "            y_confidence_list = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch in test_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                    y_confidence_list.append(y_test_pred.numpy())\n",
    "                    y_pred_tag = torch.round(y_test_pred)\n",
    "                    y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "            y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "            print(CM)\n",
    "\n",
    "            metrics_comb[urc]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics_comb[urc]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "    else:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "        model = Ensemble_Stack_Combs(no_columns=len(outputs_train_us[0]),\n",
    "                                     no_devs=len(y_train_c_us.loc[list(y_train_c_us.index)[0]]))\n",
    "        #model = Ensemble_Stack_Combs(no_columns=len(outputs_train.loc[0]), no_devs=len(y_train_c[0]))\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(list(pw_combs.values())))\n",
    "        #criterion = nn.MultiLabelSoftMarginLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(outputs_train_us), torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "            #train_data = TrainData(torch.FloatTensor(outputs_train.to_numpy()),torch.FloatTensor(y_train_c))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val_c))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.sigmoid(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "        print(CM)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "            metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                             average='macro')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    metrics_comb\n",
    "\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_BPDP_collective_LSTM_CIBE_DPP_collective.xlsx',\n",
    "                            engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_collective_LSTM_DPP_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,relevance_ths = .5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_CIBE_confidence(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,explained=False):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 8\n",
    "        negative_weights[label] = 1\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_Classifier')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    dev_trained = []\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                   torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        cum_trace_idxs = flatten_comprehension(cum_trace_idxs)\n",
    "        pref_list = flatten_comprehension(pref_list)\n",
    "        if d == dev[0]:\n",
    "            y_test_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_test_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_test_cum['prefix_length'] = pref_list\n",
    "            y_pred_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_pred_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_pred_cum['prefix_length'] = pref_list\n",
    "        y_pred_cum[str('confidence' + d)] = y_confidence_list\n",
    "        y_test_cum[d] = list(y_test)\n",
    "        y_pred_cum[d] = y_pred_list\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        to_be_checked_idx = {}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx = 0\n",
    "            for prefix in range(1, event_count[idx] + 1):\n",
    "                if prefix == 1:\n",
    "                    to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for idx in to_be_checked_idx.keys():\n",
    "                if event_count[idx] >= prefix:\n",
    "                    if prefix == 1:\n",
    "                        dev_position_pred[d][idx] = y_pred_list[to_be_checked_idx[idx][prefix - 1]][0]\n",
    "                    else:\n",
    "                        if y_pred_list[to_be_checked_idx[idx][prefix - 1]][0] == 1 and \\\n",
    "                                y_pred_list[to_be_checked_idx[idx][prefix - 2]][0] == 0:\n",
    "                            if dev_position[d][idx] <= prefix:\n",
    "                                dev_position_pred[d][idx] = dev_position[d][idx]\n",
    "                            else:\n",
    "                                dev_position_pred[d][idx] = prefix\n",
    "\n",
    "        earliness[d] = 0\n",
    "        tobe_devs = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] == 0 or dev_position_pred[d][idx] == 0:\n",
    "                continue\n",
    "            tobe_devs += 1\n",
    "            earliness[d] += dev_position_pred[d][idx] / dev_position[d][idx]\n",
    "        if not tobe_devs == 0:\n",
    "            earliness[d] = earliness[d] / tobe_devs\n",
    "\n",
    "        if explained:\n",
    "            import shap\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            np.random.seed(42)\n",
    "            e = shap.DeepExplainer(model,\n",
    "                                   torch.FloatTensor(X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]))\n",
    "\n",
    "            shap_idx = []\n",
    "            for j in range(len(y_pred_list)):\n",
    "                if y_pred_list[j][0] == y_test[j][0] == 1:\n",
    "                    shap_idx.append(j)\n",
    "            shap_values = e.shap_values(torch.FloatTensor(X_test[shap_idx]))\n",
    "            fig = shap.summary_plot(shap_values[0], X_test[shap_idx], plot_type='dot', feature_names=enc_dat.columns,\n",
    "                                    max_display=10, plot_size=(10, 5), show=False)\n",
    "            plt.savefig(path + '/ShapValues/Dev_' + z + '_' + d + '.png')\n",
    "            plt.close()\n",
    "\n",
    "            fig = shap.summary_plot(shap_values[1], X_test[shap_idx], plot_type='dot', feature_names=enc_dat.columns,\n",
    "                                    max_display=10, plot_size=(10, 5), show=False)\n",
    "            plt.savefig(path + '/ShapValues/NoDev_' + z + '_' + d + '.png')\n",
    "            plt.close()\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    avg_dev_pos = {}\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        devs = 0\n",
    "        positions = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] > 0:\n",
    "                devs += 1\n",
    "                positions += dev_position[d][idx]\n",
    "        if devs == 0:\n",
    "            continue\n",
    "        avg_dev_pos[d] = positions / devs\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    df = pd.DataFrame(data=earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness'))\n",
    "    df = pd.DataFrame(data=avg_dev_pos, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Position'))\n",
    "\n",
    "    writer.close()\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_confidence.xlsx', engine=\"xlsxwriter\")\n",
    "    for i in range(len(y_pred_cum)):\n",
    "        for d in dev_trained:\n",
    "            y_pred_cum[str('confidence' + d)][i] = y_pred_cum[str('confidence' + d)][i][0][0]\n",
    "    confidences = [.5, .6, .7, .8]\n",
    "    for confidence in confidences:\n",
    "        conf_pred = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        auc_conf_pred = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        auc_test = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        for d in dev_trained:\n",
    "            for i in range(len(y_pred_cum)):\n",
    "                if y_pred_cum[str('confidence' + d)][i] >= confidence:\n",
    "                    conf_pred[d][i] = np.array([1.0, 0.0], dtype=float)\n",
    "                else:\n",
    "                    conf_pred[d][i] = np.array([0.0, 1.0], dtype=float)\n",
    "                auc_conf_pred[d][i] = conf_pred[d][i][0]\n",
    "                auc_test[d][i] = y_test_cum[d][i][0]\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(list(y_test_cum[d]), list(conf_pred[d]))\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(list(auc_test[d]), list(auc_conf_pred[d]),\n",
    "                                                                      average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics[str('NoDev' + d)]['Support'] = CM[1][1][1]+CM[1][1][0]\n",
    "        metrics.to_excel(writer, sheet_name=(str('Unfiltered' + str(confidence))))\n",
    "\n",
    "        conf_pred = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        conf_test = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        for d in dev_trained:\n",
    "            conf_pred = y_pred_cum[\n",
    "                (y_pred_cum[str('confidence' + d)] > confidence) | (y_pred_cum[str('confidence' + d)] < 1 - confidence)]\n",
    "            conf_test = y_test_cum.loc[conf_pred.index]\n",
    "            auc_test = []\n",
    "            auc_pred = []\n",
    "            for i in conf_pred.index:\n",
    "                auc_test.append(conf_test[d][i][0])\n",
    "                auc_pred.append(conf_pred[d][i][0])\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(list(conf_test[d]), list(conf_pred[d]))\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(auc_test, auc_pred, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        metrics.to_excel(writer, sheet_name=(str('Filtered' + str(confidence))))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_CIBE_confidence(log, ref_log, aligned_traces)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def catboost_patterns(log, ref_log, aligned_traces, split=1/3, relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    max_combs_l\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "    max_combs_l\n",
    "    test_counts\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "    max_combs\n",
    "    y_cum_test[1]\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    dev_df.loc[x_test_idx].sum()[max_combs_l[0][0]]\n",
    "    pi = 4\n",
    "    print(y_cum_test_combs[pi].sum())\n",
    "    print(y_cum_test[pi].sum())\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "    y_cum_test_combs[1]\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "        #positive_weights[label] = (mean(pir.values())+statistics.stdev(pir.values()))**(1/(2*math.e))+np.log(pir[label])\n",
    "        #negative_weights[label] = (mean(nir.values())+statistics.stdev(nir.values()))**(1/(2*math.e))+np.log(nir[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/CatBoost')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MC_catoost_ES_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                      random_state=0)\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_te_c = X_cum[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr_c = X_cum[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va_c = X_cum[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "                [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "            pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "                X_train_c = x_tr_c\n",
    "                X_test_c = x_te_c\n",
    "                X_val_c = x_va_c\n",
    "                y_train_c = y_tr_c\n",
    "                y_test_c = y_te_c\n",
    "                y_val_c = y_va_c\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)\n",
    "                X_train_c = np.append(X_train_c, x_tr_c, axis=0)\n",
    "                X_test_c = np.append(X_test_c, x_te_c, axis=0)\n",
    "                X_val_c = np.append(X_val_c, x_va_c, axis=0)\n",
    "                y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "                y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "                y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(y_train_c), len(X_val), len(y_val), len(y_val_c), len(X_test), len(y_test),\n",
    "              len(y_test_c))\n",
    "\n",
    "        cat_y_train = y_train[:, 0]\n",
    "        cat_y_val = y_val[:, 0]\n",
    "        cat_y_test = y_test[:, 0]\n",
    "        cat_y_train\n",
    "        from catboost import CatBoostClassifier\n",
    "\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, scale_pos_weight=16, early_stopping_rounds=10)\n",
    "        try:\n",
    "            catboost.fit(X_train, cat_y_train)\n",
    "            y_pred = catboost.predict(X_test)\n",
    "            y_pred\n",
    "            y_preds = np.stack((y_pred, 1 - y_pred), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "\n",
    "            print(CM)\n",
    "\n",
    "            to_be_checked_idx = {}\n",
    "            for idx in x_test_idx:\n",
    "                cum_idx = 0\n",
    "                for prefix in range(1, event_count[idx] + 1):\n",
    "                    if prefix == 1:\n",
    "                        to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                    else:\n",
    "                        to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                    cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "            y_output_train = catboost.predict_proba(X_train_c)\n",
    "            y_output_test = catboost.predict_proba(X_test_c)\n",
    "            y_output_val = catboost.predict_proba(X_val_c)\n",
    "\n",
    "            outputs_train['NoDev' + str(d)] = y_output_train[:, 0]\n",
    "            outputs_train['Dev' + str(d)] = y_output_train[:, 1]\n",
    "            outputs_test['NoDev' + str(d)] = y_output_test[:, 0]\n",
    "            outputs_test['Dev' + str(d)] = y_output_test[:, 1]\n",
    "            outputs_val['NoDev' + str(d)] = y_output_val[:, 0]\n",
    "            outputs_val['Dev' + str(d)] = y_output_val[:, 1]\n",
    "            if d == dev[0]:\n",
    "                outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "                outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "                outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "\n",
    "        except Exception as er:\n",
    "            metrics[d] = er\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()\n",
    "    outputs_test\n",
    "    y_test_c.sum()\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    outputs_train\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for i, label in enumerate(unique_rel_combs):\n",
    "        positives[label] = y_train_c[:, i].sum()\n",
    "        negatives[label] = len(y_train_c) - y_train_c[:, i].sum()\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pir[label] = min((max(positives[label], negatives[label]) / positives[label]), 10000)\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    pw_combs = {}\n",
    "    nw_combs = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pw_combs[label] = min((mean(pir.values()) ** (4 / (2 ** math.e)) + (np.log(pirlbl[label]))), 200)\n",
    "        #pw_combs[label] =  min(2*(mean(pir.values()) ** ((2* math.e)) + (np.log(pirlbl[label]))), 100000)\n",
    "        nw_combs[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    pw_combs\n",
    "    metrics_comb\n",
    "    if len(y_train_c[0]) == 1:\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, scale_pos_weight=list(pw_combs.values())[0],\n",
    "                                      early_stopping_rounds=10)\n",
    "        try:\n",
    "            catboost.fit(outputs_train, y_train_c)\n",
    "            y_pred = catboost.predict(outputs_test)\n",
    "            y_pred\n",
    "            y_preds = np.stack((y_pred, 1 - y_pred), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "            y_test_c = np.column_stack((y_test_c, 1 - y_test_c))\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "            print(CM)\n",
    "            for i, urc in enumerate(unique_rel_combs):\n",
    "                metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "                metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "                metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "                try:\n",
    "                    metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i],\n",
    "                                                                                 np.array(y_pred_list)[:, i],\n",
    "                                                                                 average='macro')\n",
    "                except Exception as er:\n",
    "                    metrics_comb[urc]['ROC_AUC'] = er\n",
    "                metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "                metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "                metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "        except Exception as er:\n",
    "            metrics_comb[unique_rel_combs[0]] = er\n",
    "    else:\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, loss_function='MultiLogloss',\n",
    "                                      early_stopping_rounds=10)\n",
    "        try:\n",
    "            metric_combs = unique_rel_combs\n",
    "            for ri in range(len(y_train_c[0]) - 1, -1, -1):\n",
    "                if y_train_c[:, ri].sum() == 0:\n",
    "                    y_train_c = np.delete(y_train_c, ri, 1)\n",
    "                    y_test_c = np.delete(y_test_c, ri, 1)\n",
    "                    del metric_combs[ri]\n",
    "\n",
    "            catboost.fit(outputs_train, y_train_c)\n",
    "            y_pred = catboost.predict(outputs_test)\n",
    "\n",
    "            y_pred_list = y_pred.tolist()\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "            print(CM)\n",
    "            for i, urc in enumerate(metric_combs):\n",
    "                metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "                metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "                metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "                try:\n",
    "                    metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i],\n",
    "                                                                                 np.array(y_pred_list)[:, i],\n",
    "                                                                                 average='macro')\n",
    "                except Exception as er:\n",
    "                    metrics_comb[urc]['ROC_AUC'] = er\n",
    "                metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "                metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "                metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "\n",
    "        except Exception as er:\n",
    "            metrics_comb[unique_rel_combs[0]] = er\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_catboost.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## load feature vectors from MPPN\n",
    "file = ask_for_path(REL_INPUT_PATH, 17)  # adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    pd_cases_fv = pickle.load(f)\n",
    "\n",
    "\n",
    "def IDP_separate_MPPN(log, pd_cases_fv, aligned_traces, split=1 / 3, u_sample=True, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "    X_cum = {}\n",
    "    for pref in range(1, max_ev + 1):\n",
    "        X_cum[pref] = pd.DataFrame(columns=['FV'], index=list(range(len(pd_cases_fv['case:concept:name'].unique()))))\n",
    "    no_cases = len(pd_cases_fv['case:concept:name'].unique())\n",
    "    row = 0\n",
    "    for counter in range(len(pd_cases_fv)):\n",
    "        X_cum[len(pd_cases_fv.loc[counter].trace)]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "        if not counter == len(pd_cases_fv) - 1:\n",
    "            if not pd_cases_fv.loc[counter]['case:concept:name'] == pd_cases_fv.loc[counter + 1]['case:concept:name']:\n",
    "                if not len(pd_cases_fv.loc[counter].trace) == max_ev:\n",
    "                    for missing in range(len(pd_cases_fv.loc[counter].trace) + 1, max_ev + 1):\n",
    "                        X_cum[missing]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "                row += 1\n",
    "        else:\n",
    "            if not len(pd_cases_fv.loc[counter].trace) == max_ev:\n",
    "                for missing in range(len(pd_cases_fv.loc[counter].trace) + 1, max_ev + 1):\n",
    "                    X_cum[missing]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "            row += 1\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_MPPN_Classifier')\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_MPPN_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                        test_size=split, random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    dev_trained = []\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "        if dev_distribution[d]['Test'] / len(x_test_idx) <= 0.05:\n",
    "            metrics[d]['Notes'] = str(\n",
    "                'Only very few deviations in Test Set:' + str(dev_distribution[d]['Test'] / len(x_test_idx)))\n",
    "\n",
    "        elif dev_distribution[d]['Training'] / len(x_train_idx) <= 0.05:\n",
    "            metrics[d]['Notes'] = str(\n",
    "                'Only very few deviations in Training Set:' + str(dev_distribution[d]['Training'] / len(x_train_idx)))\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = pd.DataFrame(X_cum[1]['FV'].tolist()).add_prefix(\"c\")\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            P = pd.DataFrame(X_cum[prefix]['FV'].tolist()).add_prefix(\"c\")\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = P.loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = P.loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = P.loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "        LEARNING_RATE = 0.0001\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=128)\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                   torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "\n",
    "        to_be_checked_idx = {}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx = 0\n",
    "            for prefix in range(1, event_count[idx] + 1):\n",
    "                if prefix == 1:\n",
    "                    to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for idx in to_be_checked_idx.keys():\n",
    "                if event_count[idx] >= prefix:\n",
    "                    if prefix == 1:\n",
    "                        dev_position_pred[d][idx] = y_pred_list[to_be_checked_idx[idx][prefix - 1]][0]\n",
    "                    else:\n",
    "                        if y_pred_list[to_be_checked_idx[idx][prefix - 1]][0] == 1 and \\\n",
    "                                y_pred_list[to_be_checked_idx[idx][prefix - 2]][0] == 0:\n",
    "                            if dev_position[d][idx] <= prefix:\n",
    "                                dev_position_pred[d][idx] = dev_position[d][idx]\n",
    "                            else:\n",
    "                                dev_position_pred[d][idx] = prefix\n",
    "\n",
    "        earliness[d] = 0\n",
    "        tobe_devs = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] == 0 or dev_position_pred[d][idx] == 0:\n",
    "                continue\n",
    "            tobe_devs += 1\n",
    "            earliness[d] += dev_position_pred[d][idx] / dev_position[d][idx]\n",
    "        if not tobe_devs == 0:\n",
    "            earliness[d] = earliness[d] / tobe_devs\n",
    "\n",
    "    avg_dev_pos = {}\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        devs = 0\n",
    "        positions = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] > 0:\n",
    "                devs += 1\n",
    "                positions += dev_position[d][idx]\n",
    "        if devs == 0:\n",
    "            continue\n",
    "        avg_dev_pos[d] = positions / devs\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    df = pd.DataFrame(data=earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness'))\n",
    "    df = pd.DataFrame(data=avg_dev_pos, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Position'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def genga_benchmark(log, aligned_traces, c=2, alpha=1,split = 1 / 3):\n",
    "    xt,z =os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "    #if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "    for j, trace in enumerate(log):\n",
    "        for i, event in enumerate(trace):\n",
    "            pi = i + 1\n",
    "            if pi == 1:\n",
    "                event['duration'] = 0\n",
    "                event['trace'] = event['concept:name']\n",
    "\n",
    "\n",
    "            elif pi <= event_count[j]:\n",
    "                event['duration'] = (log[j][i]['time:timestamp'] - log[j][0]['time:timestamp']).total_seconds() / 60\n",
    "                event['trace'] = str(log[j][i - 1]['trace'] + ', ' + event['concept:name'])\n",
    "        trace.attributes['duration'] = log[j][event_count[j] - 1]['duration']\n",
    "    tree_df = pm4py.convert_to_dataframe(log)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_dataframe = tree_df.filter(items=['case:AMOUNT_REQ', 'duration'], axis=1)\n",
    "        ref_clean_dat = ref_dataframe.rename(columns={'case:AMOUNT_REQ': 'AMOUNT_REQ'})\n",
    "    else:\n",
    "        ref_clean_dat = tree_df.filter(items=['duration'], axis=1)\n",
    "\n",
    "    X_tree = ref_clean_dat.loc[x_train_idx]\n",
    "    for d in dev:\n",
    "        y_tree = dev_df[d][x_train_idx]\n",
    "\n",
    "        from feature_engine.discretisation import DecisionTreeDiscretiser\n",
    "\n",
    "        disc = DecisionTreeDiscretiser(regression=False)\n",
    "\n",
    "        # fit the transformer\n",
    "        disc.fit(X_tree, y_tree)\n",
    "\n",
    "        tree_df = pm4py.convert_to_dataframe(log)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            X_preprosessed = tree_df.filter(items=['case:AMOUNT_REQ', 'duration'], axis=1)\n",
    "            X_preprosessed = X_preprosessed.rename(columns={'case:AMOUNT_REQ': 'AMOUNT_REQ'})\n",
    "        else:\n",
    "            X_preprosessed = tree_df.filter(items=['duration'], axis=1)\n",
    "\n",
    "        try:\n",
    "            X_preprosessed = disc.transform(X_preprosessed)\n",
    "        except Exception as er:\n",
    "            print(er)\n",
    "\n",
    "\n",
    "        genga_states = []\n",
    "        runner = 0\n",
    "        for j, trace in enumerate(log):\n",
    "            for i, event in enumerate(trace):\n",
    "                if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "                    event['genga'] = str(\n",
    "                        event['trace'] + '_' + str(X_preprosessed['AMOUNT_REQ'][runner + i]) + '_' + str(\n",
    "                            X_preprosessed['duration'][runner + i]))\n",
    "                    if not event['genga'] in genga_states:\n",
    "                        genga_states.append(event['genga'])\n",
    "                else:\n",
    "                    event['genga'] = str(\n",
    "                        event['trace'] + '_' + str(\n",
    "                            X_preprosessed['duration'][runner + i]))\n",
    "                    if not event['genga'] in genga_states:\n",
    "                        genga_states.append(event['genga'])\n",
    "            runner += event_count[j]\n",
    "\n",
    "        len(genga_states)\n",
    "        genga_counts = pd.DataFrame(data=0, index=genga_states, columns=[d])\n",
    "        genga_counts['count'] = 0\n",
    "        for j, trace in enumerate(log):\n",
    "            if not j in x_train_idx:\n",
    "                continue\n",
    "            for i, event in enumerate(trace):\n",
    "                genga_counts[d][event['genga']] += dev_df[d][j]\n",
    "                genga_counts['count'][event['genga']] += 1\n",
    "        genga_counts['count'].sum()\n",
    "        test_count = 0\n",
    "        for idx in y_test_idx:\n",
    "            test_count += event_count[idx]\n",
    "        y_true_df = pd.DataFrame(data=0, index=range(test_count), columns=[d])\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max(event_count.values()) + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "\n",
    "        runner = 0\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                y_true_df[d][runner] = y_cum_test[prefix][d][enumerated_trace_idx[prefix][idx]]\n",
    "                runner += 1\n",
    "\n",
    "        genga_states_test = {}\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            genga_states_test[prefix] = []\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                genga_states_test[prefix].append(log[enumerated_trace_idx[prefix][idx]][prefix - 1]['genga'])\n",
    "\n",
    "        y_pred_df = pd.DataFrame(data=0, index=range(test_count), columns=[d])\n",
    "\n",
    "        runner = 0\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                p = genga_counts[d][genga_states_test[prefix][idx]]\n",
    "                n = genga_counts['count'][genga_states_test[prefix][idx]] - genga_counts[d][genga_states_test[prefix][idx]]\n",
    "                xb = p / (p + n + c)\n",
    "                xd = n / (p + n + c)\n",
    "                xu = c / (p + n + c)\n",
    "                if (xu < xb or xu < xd) and xb > alpha * xd:\n",
    "                    y_pred_df[d][runner] = 1\n",
    "                runner += 1\n",
    "        y_test_list = y_true_df.values.tolist()\n",
    "        y_pred_list = y_pred_df.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "        CM = sklearn.metrics.confusion_matrix(y_test_list, y_pred_list)\n",
    "\n",
    "\n",
    "        try:\n",
    "            metrics[d]['Precision'] = CM[1][1] / (CM[1][1] + CM[0][1])\n",
    "            metrics[d]['Recall'] = CM[1][1] / (CM[1][1] + CM[1][0])\n",
    "        except Exception as er:\n",
    "             metrics[d]['Precision']=CM[0]\n",
    "             metrics[d]['Recall']=er\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_true_df[d], y_pred_df[d], average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        try:\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[0][0] / (CM[0][0] + CM[1][0])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[0][0] / (CM[0][0] + CM[0][1])\n",
    "        except Exception as er:\n",
    "            metrics[str('NoDev' + d)]['ROC_AUC'] = er\n",
    "\n",
    "\n",
    "        to_be_checked_idx = {}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx = 0\n",
    "            for prefix in range(1, event_count[idx] + 1):\n",
    "                if prefix == 1:\n",
    "                    to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "    path = (os.getcwd() + '/Genga')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_Genga_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classify_cat(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "\n",
    "    # print(len(dev), dev)\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "        #positive_weights[label] = (mean(pir.values())+statistics.stdev(pir.values()))**(1/(2*math.e))+np.log(pir[label])\n",
    "        #negative_weights[label] = (mean(nir.values())+statistics.stdev(nir.values()))**(1/(2*math.e))+np.log(nir[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/CatBoost')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MC_catoost_ES_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        cat_y_train = y_train[:, 0]\n",
    "        cat_y_val = y_val[:, 0]\n",
    "        cat_y_test = y_test[:, 0]\n",
    "        cat_y_train\n",
    "        from catboost import CatBoostClassifier\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, scale_pos_weight=16, early_stopping_rounds=10)\n",
    "        try:\n",
    "            catboost.fit(X_train, cat_y_train)\n",
    "            y_pred = catboost.predict(X_test)\n",
    "            y_pred\n",
    "            y_preds = np.stack((y_pred, 1 - y_pred), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "\n",
    "            print(CM)\n",
    "\n",
    "            to_be_checked_idx = {}\n",
    "            for idx in x_test_idx:\n",
    "                cum_idx = 0\n",
    "                for prefix in range(1, event_count[idx] + 1):\n",
    "                    if prefix == 1:\n",
    "                        to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                    else:\n",
    "                        to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                    cum_idx += len(enumerated_trace_idx[prefix])\n",
    "        except Exception as er:\n",
    "            metrics[d]=er\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classify_xgb(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "\n",
    "    # print(len(dev), dev)\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "        #positive_weights[label] = (mean(pir.values())+statistics.stdev(pir.values()))**(1/(2*math.e))+np.log(pir[label])\n",
    "        #negative_weights[label] = (mean(nir.values())+statistics.stdev(nir.values()))**(1/(2*math.e))+np.log(nir[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/CatBoost')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MC_catoost_ES_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        cxgb_y_train = y_train[:, 0]\n",
    "        xgb_y_val = y_val[:, 0]\n",
    "        xgb_y_test = y_test[:, 0]\n",
    "        xgb_y_train\n",
    "        bst = xgb.XGBClassifier(max_depth=16, scale_pos_weight=16)\n",
    "\n",
    "        try:\n",
    "            bst.fit(X_train, xgb_y_train, eval_set=[(X_train, xgb_y_train), (X_val, xgb_y_val)], early_stopping_rounds=10)\n",
    "            y_preds = bst.predict(X_test)\n",
    "            y_preds\n",
    "            y_preds = np.stack((y_preds, 1 - y_preds), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "\n",
    "            print(CM)\n",
    "\n",
    "            to_be_checked_idx = {}\n",
    "            for idx in x_test_idx:\n",
    "                cum_idx = 0\n",
    "                for prefix in range(1, event_count[idx] + 1):\n",
    "                    if prefix == 1:\n",
    "                        to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                    else:\n",
    "                        to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                    cum_idx += len(enumerated_trace_idx[prefix])\n",
    "        except Exception as er:\n",
    "            metrics[d]=er\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Suffix Prediction: Load Suffixes and execute prediction of deviations\n",
    "file= ask_for_path(REL_INPUT_PATH,15)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    suffixes=pickle.load(f)\n",
    "def suffix_prediction_deviations(log, aligned_traces, net, initial_marking, final_marking, suffixes,split = 1 / 3):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    if z == 'MPPN_BPIC_2020_request_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_international_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_prepaid_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_domestic_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    suffixes = suffixes.rename(columns={'case:concept:name': 'case:trace_ID', 'IDX': 'case:IDX'})\n",
    "    suffixes['case:concept:name'] = suffixes[[\"case:trace_ID\", \"case:IDX\"]].astype(str).apply(\"_\".join, axis=1)\n",
    "\n",
    "    suffixes['case:prefix'] = 1\n",
    "    for suf in range(1, len(suffixes)):\n",
    "        if suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and suffixes['case:IDX'][suf - 1] == \\\n",
    "                suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1]\n",
    "        elif suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and not suffixes['case:IDX'][suf - 1] == \\\n",
    "                                                                                          suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1] + 1\n",
    "\n",
    "    events_per_trace = pd.DataFrame(data=0, columns=['count'], index=suffixes['case:IDX'].unique())\n",
    "    for idx in range(len(suffixes)):\n",
    "        events_per_trace['count'][suffixes['case:IDX'][idx]] += 1\n",
    "\n",
    "    predicted_log = pm4py.convert_to_event_log(suffixes)\n",
    "\n",
    "\n",
    "    aligned_predictions = pm4py.conformance_diagnostics_alignments(predicted_log, net, initial_marking, final_marking)\n",
    "\n",
    "    i = 0\n",
    "    pred_dev = []\n",
    "    for trace in predicted_log:\n",
    "        no_moves = len(aligned_predictions[i]['alignment'])\n",
    "        for j in range(0, len(aligned_predictions[i]['alignment'])):\n",
    "            if aligned_predictions[i]['alignment'][j][1] == None or aligned_predictions[i]['alignment'][j][0] == \\\n",
    "                    aligned_predictions[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                if not str(aligned_predictions[i]['alignment'][j]) in pred_dev:\n",
    "                    pred_dev.append(str(aligned_predictions[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    dev_df['trace_ID'] = 0\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        dev_df['trace_ID'][k] = trace.attributes['concept:name']\n",
    "        k += 1\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    caseIDs_test = []\n",
    "    key_caseID_testIDX = {}\n",
    "    for idx in x_test_idx:\n",
    "        caseIDs_test.append(log[idx].attributes['concept:name'])\n",
    "        key_caseID_testIDX[log[idx].attributes['concept:name']] = idx\n",
    "    caseIDs_train = []\n",
    "    for idx in x_train_idx:\n",
    "        caseIDs_train.append(log[idx].attributes['concept:name'])\n",
    "    y_cum_pred = {}\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_pred[\n",
    "            ev] = pd.DataFrame(data=0, columns=dev, index=caseIDs_test)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].set_index('trace_ID')\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].drop(index=caseIDs_train)\n",
    "    for i, alignment in enumerate(aligned_predictions):\n",
    "        no_moves = len(alignment['alignment'])\n",
    "        for d in dev:\n",
    "            for j in range(no_moves):\n",
    "                if str(aligned_predictions[i]['alignment'][j]) == d:\n",
    "                    #print(i, predicted_log[i].attributes['trace_ID'],d, predicted_log[i].attributes['prefix'])\n",
    "                    y_cum_pred[predicted_log[i].attributes['prefix']][d][predicted_log[i].attributes['trace_ID']] = 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in caseIDs_test:\n",
    "            if event_count[key_caseID_testIDX[trace_idx]] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].drop(drop_idx)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[ev] = y_cum_test[ev].sort_index()\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].sort_index()\n",
    "\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        if ev == 1:\n",
    "            y_pred_list = y_cum_pred[ev]\n",
    "            y_test = y_cum_test[ev]\n",
    "        else:\n",
    "\n",
    "            y_pred_list = np.append(y_pred_list, y_cum_pred[ev], axis=0)\n",
    "            y_test = np.append(y_test, y_cum_test[ev], axis=0)\n",
    "    len(y_cum_test[1].values.tolist())\n",
    "    print(len(y_pred_list))\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                       average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "    path = (os.getcwd() + '/Suffix_Prediction')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_suffix_pred_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file= ask_for_path(REL_INPUT_PATH,14)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    suffixes=pickle.load(f)\n",
    "def suffix_prediction_patterns(log, aligned_traces, net, initial_marking, final_marking, suffixes,split = 1 / 3, relevance_ths = 0.5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    if z == 'MPPN_BPIC_2020_request_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf] = str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_international_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf] = str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_prepaid_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf] = str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_domestic_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf] = str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    suffixes = suffixes.rename(columns={'case:concept:name': 'case:trace_ID', 'IDX': 'case:IDX'})\n",
    "    suffixes['case:concept:name'] = suffixes[[\"case:trace_ID\", \"case:IDX\"]].astype(str).apply(\"_\".join, axis=1)\n",
    "\n",
    "    suffixes['case:prefix'] = 1\n",
    "    for suf in range(1, len(suffixes)):\n",
    "        if suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and suffixes['case:IDX'][suf - 1] == \\\n",
    "                suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1]\n",
    "        elif suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and not suffixes['case:IDX'][suf - 1] == \\\n",
    "                                                                                          suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1] + 1\n",
    "\n",
    "    events_per_trace = pd.DataFrame(data=0, columns=['count'], index=suffixes['case:IDX'].unique())\n",
    "    for idx in range(len(suffixes)):\n",
    "        events_per_trace['count'][suffixes['case:IDX'][idx]] += 1\n",
    "\n",
    "    predicted_log = pm4py.convert_to_event_log(suffixes)\n",
    "\n",
    "    aligned_predictions = pm4py.conformance_diagnostics_alignments(predicted_log, net, initial_marking, final_marking)\n",
    "\n",
    "    i = 0\n",
    "    pred_dev = []\n",
    "    for trace in predicted_log:\n",
    "        no_moves = len(aligned_predictions[i]['alignment'])\n",
    "        for j in range(0, len(aligned_predictions[i]['alignment'])):\n",
    "            if aligned_predictions[i]['alignment'][j][1] == None or aligned_predictions[i]['alignment'][j][0] == \\\n",
    "                    aligned_predictions[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                if not str(aligned_predictions[i]['alignment'][j]) in pred_dev:\n",
    "                    pred_dev.append(str(aligned_predictions[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "\n",
    "    dev\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    dev_df['trace_ID'] = 0\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        dev_df['trace_ID'][k] = trace.attributes['concept:name']\n",
    "        k += 1\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    max_combs_l\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "    max_combs_l\n",
    "    test_counts\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "    max_combs\n",
    "    y_cum_test[1]\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    dev_df.loc[x_test_idx].sum()[max_combs_l[0][0]]\n",
    "    pi = 4\n",
    "    print(y_cum_test_combs[pi].sum())\n",
    "    print(y_cum_test[pi].sum())\n",
    "    y_cum_test_o_combs = {}\n",
    "    columns_needed = list(max_combs.keys())\n",
    "    columns_needed.append('trace_ID')\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][columns_needed]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "    y_cum_test_combs[1]\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    caseIDs_test = []\n",
    "    key_caseID_testIDX = {}\n",
    "    for idx in x_test_idx:\n",
    "        caseIDs_test.append(log[idx].attributes['concept:name'])\n",
    "        key_caseID_testIDX[log[idx].attributes['concept:name']] = idx\n",
    "    caseIDs_train = []\n",
    "    for idx in x_train_idx:\n",
    "        caseIDs_train.append(log[idx].attributes['concept:name'])\n",
    "    y_cum_pred = {}\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_pred[\n",
    "            ev] = pd.DataFrame(data=0, columns=dev, index=caseIDs_test)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].set_index('trace_ID')\n",
    "        y_cum_test_o_combs[ev] = y_cum_test_o_combs[ev].set_index('trace_ID')\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].drop(index=caseIDs_train)\n",
    "        y_cum_test_o_combs[\n",
    "            ev] = y_cum_test_o_combs[\n",
    "            ev].drop(index=caseIDs_train)\n",
    "    for i, alignment in enumerate(aligned_predictions):\n",
    "        no_moves = len(alignment['alignment'])\n",
    "        for d in dev:\n",
    "            for j in range(no_moves):\n",
    "                if str(aligned_predictions[i]['alignment'][j]) == d:\n",
    "                    #print(i, predicted_log[i].attributes['trace_ID'],d, predicted_log[i].attributes['prefix'])\n",
    "                    y_cum_pred[predicted_log[i].attributes['prefix']][d][predicted_log[i].attributes['trace_ID']] = 1\n",
    "\n",
    "    y_cum_pred_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_pred_combs[prefix] = y_cum_pred[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_pred_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_pred_combs[prefix].index):\n",
    "                if event_count[key_caseID_testIDX[i]] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_pred_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_pred_combs[prefix][j][i] = 0\n",
    "                    y_cum_pred_combs[prefix][comb][i] = 1\n",
    "    y_cum_pred_combs\n",
    "    y_cum_pred_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_pred_o_combs[prefix] = y_cum_pred_combs[prefix][list(max_combs.keys())]\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in caseIDs_test:\n",
    "            if event_count[key_caseID_testIDX[trace_idx]] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].drop(drop_idx)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[ev] = y_cum_test[ev].sort_index()\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].sort_index()\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        if ev == 1:\n",
    "            y_pred_list = y_cum_pred[ev]\n",
    "            y_test = y_cum_test[ev]\n",
    "        else:\n",
    "\n",
    "            y_pred_list = np.append(y_pred_list, y_cum_pred[ev], axis=0)\n",
    "            y_test = np.append(y_test, y_cum_test[ev], axis=0)\n",
    "    len(y_cum_test[1].values.tolist())\n",
    "    print(len(y_pred_list))\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                       average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "    path = (os.getcwd() + '/Suffix_Prediction')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_suffix_pred_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()\n",
    "    len(y_pred_list)\n",
    "    metrics\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in caseIDs_test:\n",
    "            if event_count[key_caseID_testIDX[trace_idx]] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_pred_o_combs[ev] = y_cum_pred_o_combs[ev].drop(drop_idx)\n",
    "        y_cum_test_o_combs[ev] = y_cum_test_o_combs[ev].drop(drop_idx)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[ev] = y_cum_test_o_combs[ev].sort_index()\n",
    "        y_cum_pred_o_combs[ev] = y_cum_pred_o_combs[ev].sort_index()\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        if ev == 1:\n",
    "            y_pred_list = y_cum_pred_o_combs[ev]\n",
    "            y_test = y_cum_test_o_combs[ev]\n",
    "        else:\n",
    "            y_pred_list = np.append(y_pred_list, y_cum_pred_o_combs[ev], axis=0)\n",
    "            y_test = np.append(y_test, y_cum_test_o_combs[ev], axis=0)\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "    print(CM)\n",
    "\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    metrics_comb\n",
    "\n",
    "    for i, urc in enumerate(unique_rel_combs):\n",
    "        metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                         average='macro')\n",
    "        except Exception as er:\n",
    "            metrics_comb[urc]['ROC_AUC'] = er\n",
    "        metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "        metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_suffix.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "suffix_prediction_patterns(log, aligned_traces, net, initial_marking, final_marking, suffixes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REL_INPUT_PATH = \"/../mppn_cms/\" # here lie the event logs (.csv), the to-be model (.bpmn) and the already aligned traces (.pkl)\n",
    "file= ask_for_path(REL_INPUT_PATH,7)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    cm=pickle.load(f)\n",
    "def get_metrics_IDP_MPPN(cm):\n",
    "    metrics = pd.DataFrame(data=0, columns=list(cm.keys()), index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for i, d in enumerate(list(cm.keys())):\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "        metrics[str('NoDev' + d)]['Precision'] = cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] / (\n",
    "                    cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] + cm[list(cm.keys())[i]]['conf_matrix'][0][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] / (\n",
    "                    cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] + cm[list(cm.keys())[i]]['conf_matrix'][0][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] + \\\n",
    "                                cm[list(cm.keys())[i]]['conf_matrix'][0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(cm[list(cm.keys())[i]]['targets'],\n",
    "                                                                  cm[list(cm.keys())[i]]['predictions'], average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[d]['Precision'] = cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] / (\n",
    "                    cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] + cm[list(cm.keys())[i]]['conf_matrix'][1][0][1])\n",
    "        metrics[d]['Recall'] = cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] / (\n",
    "                    cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] + cm[list(cm.keys())[i]]['conf_matrix'][1][1][0])\n",
    "        metrics[d]['Support'] = cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] + \\\n",
    "                                               cm[list(cm.keys())[i]]['conf_matrix'][1][1][0]\n",
    "    metrics\n",
    "    path = (os.getcwd() + '/MPPN2End')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MPPN2End.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()\n",
    "get_metrics_IDP_MPPN(cm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Here start the comparisons to other design choices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_no_imbalance(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt,z =os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i=0\n",
    "    dev=[] # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i+=1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test={} # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df=pd.DataFrame(data=0,columns=dev, index=range(len(log))) # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order={} # dict with event sequences for each trace\n",
    "    event_count={} # dict with trace length for each trace\n",
    "    max_ev=0 # will be maximum trace length\n",
    "    k=0\n",
    "    for trace in log:\n",
    "        event_order[k]=[]\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "        event_count[k]=len(event_order[k])\n",
    "        k+=1\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i]=1\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1,max_ev+1):\n",
    "        y_cum_test[ev]=dev_df.copy() # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< ev:\n",
    "                drop_idx.append(trace_idx) # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev]=y_cum_test[ev].drop(drop_idx)\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        j=no_moves-1 # iterator over moves in alignment, starting at the end\n",
    "        m=len(event_order[i]) # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >=0:\n",
    "            if aligned_traces[i]['alignment'][j][1] == None: # if silent move, just go one move further to the beginning in the alignment\n",
    "                j-=1\n",
    "            elif aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j-=1\n",
    "                    m-=1\n",
    "            elif event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # log move detected\n",
    "                for q in range(m,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j-=1\n",
    "                m-=1\n",
    "            elif m==max_ev:\n",
    "                j-=1\n",
    "            else: # model move deteceted\n",
    "                for q in range(m+1,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j-=1\n",
    "        i+=1\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe=ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe=ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe=ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe=ref_dataframe.reset_index()\n",
    "    ref_raw_dat=ref_dataframe.drop('index', axis=1)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ']= pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat=ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z=='aligned_traces_20int.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20dom.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "    elif z=='aligned_traces_20prep.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20RfP.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat=ref_raw_dat.copy()\n",
    "    ref_enc_dat=pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    min_pref=1\n",
    "    max_pref=max_ev\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE=0.0001\n",
    "\n",
    "    X_cum={}\n",
    "    metrics=pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        complex_index_encoding(log,prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe=dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe=dataframe.filter(like='case:', axis=1)\n",
    "        dataframe=dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe=dataframe.reset_index()\n",
    "        raw_dat=dataframe.drop('index', axis=1)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ']= pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat=raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z=='aligned_traces_20int.pkl':\n",
    "            clean_dat=raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20dom.pkl':\n",
    "            clean_dat=raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "        elif z=='aligned_traces_20prep.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20RfP.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat=raw_dat.copy()\n",
    "        enc_dat=pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key]=0 # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "        enc_dat=pd.DataFrame(data=imp.fit_transform(enc_dat),columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix]=enc_dat.copy()\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix]=X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    import math\n",
    "\n",
    "\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev'+d)]=0\n",
    "\n",
    "    path=(os.getcwd()+'/BPDP_Classifier')\n",
    "    xt,z =os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path+'/'+z+'_MC_CrossEntropyLoss_NoImbalance_'+str(early_stop)+'_'+str(round(split,2))+'.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "\n",
    "        Y_cum_dev={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            Y_cum_dev[prefix]=pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev']=0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i]=1-y_cum_test[prefix][d][i]\n",
    "            if prefix==1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "\n",
    "        x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "        print('index length ', len(x_train_idx),len(x_test_idx),len(y_train_idx),len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "        enumerated_trace_idx={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            drop_idx=[]\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx]< prefix:\n",
    "                    drop_idx.append(trace_idx) # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te=X_cum[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr=X_cum[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va=X_cum[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te=Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr=Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va=Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix]=list(set(y_test_idx)-set(drop_idx))\n",
    "            print('subset length ',prefix, len(x_te),len(x_tr),len(y_te),len(y_tr))\n",
    "\n",
    "            if prefix ==1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append( X_train, x_tr, axis=0)\n",
    "                X_test = np.append( X_test, x_te, axis=0)\n",
    "                y_train = np.append( y_train, y_tr, axis=0)\n",
    "                y_test = np.append( y_test, y_te, axis=0)\n",
    "                y_val = np.append( y_val, y_va, axis=0)\n",
    "                X_val = np.append( X_val, x_va, axis=0)# combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train),len(y_train),len(X_val),len(y_val),len(X_test),len(y_test))\n",
    "\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if not early_stop:\n",
    "\n",
    "            print(dev, 'training start no ES')\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            for e in range(1, EPOCHS+1):\n",
    "                epoch_loss = 0\n",
    "                epoch_acc = 0\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    y_pred = model(X_batch)\n",
    "\n",
    "                    loss = criterion(y_pred.unsqueeze(1), y_batch.unsqueeze(1))\n",
    "                    acc = binary_acc(y_pred.unsqueeze(1), y_batch.unsqueeze(1))/len(y_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "                print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} |', d)\n",
    "\n",
    "        else:\n",
    "            print('training start with ES')\n",
    "            EPOCHS=300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch<EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss=0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device))/len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "                    epoch_loss+=loss\n",
    "                    if i == len(steps)-1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model,vloss): done = True\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss: {epoch_loss/len(train_loader)}, Acc: {epoch_acc/len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss {epoch_loss/len(train_loader):}, Acc: {epoch_acc/len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision']=CM[0][1][1]/(CM[0][1][1]+CM[0][0][1])\n",
    "        metrics[d]['Recall']=CM[0][1][1]/(CM[0][1][1]+CM[0][1][0])\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] =  sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev'+d)]['Precision']=CM[1][1][1]/(CM[1][1][1]+CM[1][0][1])\n",
    "        metrics[str('NoDev'+d)]['Recall']=CM[1][1][1]/(CM[1][1][1]+CM[1][1][0])\n",
    "        print(CM)\n",
    "\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_collective_CIBE(log, ref_log, aligned_traces, u_sample=True, split=1/3, early_stop=True):\n",
    "    xt,z =os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i=0\n",
    "    dev=[] # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i+=1\n",
    "\n",
    "    y_cum_test={} # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df=pd.DataFrame(data=0,columns=dev, index=range(len(log))) # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order={} # dict with event sequences for each trace\n",
    "    event_count={} # dict with trace length for each trace\n",
    "    max_ev=0 # will be maximum trace length\n",
    "    k=0\n",
    "    for trace in log:\n",
    "        event_order[k]=[]\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "        event_count[k]=len(event_order[k])\n",
    "        k+=1\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i]=1\n",
    "        i+=1\n",
    "    for ev in range(1,max_ev+1):\n",
    "        y_cum_test[ev]=dev_df.copy() # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< ev:\n",
    "                drop_idx.append(trace_idx) # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev]=y_cum_test[ev].drop(drop_idx)\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        j=no_moves-1 # iterator over moves in alignment, starting at the end\n",
    "        m=len(event_order[i]) # iterator over event sequence, starting at the end\n",
    "        while j >=0:\n",
    "            if aligned_traces[i]['alignment'][j][1] == None: # if silent move, just go one move further to the beginning in the alignment\n",
    "                j-=1\n",
    "            elif aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j-=1\n",
    "                    m-=1\n",
    "            elif event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # log move detected\n",
    "                for q in range(m,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j-=1\n",
    "                m-=1\n",
    "            elif m==max_ev:\n",
    "                j-=1\n",
    "            else: # model move deteceted\n",
    "                for q in range(m+1,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j-=1\n",
    "        i+=1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000) # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe=ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe=ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe=ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe=ref_dataframe.reset_index()\n",
    "    ref_raw_dat=ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ']= pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat=ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z=='aligned_traces_20int.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20dom.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "    elif z=='aligned_traces_20prep.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20RfP.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat=ref_raw_dat.copy()\n",
    "    ref_enc_dat=pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "    X_cum={}\n",
    "    metrics=pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC','LenTrain', 'LenTrain_beforeUS_0', 'LenTrain_beforeUS_1', 'LenTrain_afterUS_0', 'LenTrain_afterUS_1'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        complex_index_encoding(log,prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe=dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe=dataframe.filter(like='case:', axis=1)\n",
    "        dataframe=dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe=dataframe.reset_index()\n",
    "        raw_dat=dataframe.drop('index', axis=1)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ']= pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat=raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z=='aligned_traces_20int.pkl':\n",
    "            clean_dat=raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20dom.pkl':\n",
    "            clean_dat=raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "        elif z=='aligned_traces_20prep.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20RfP.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat=raw_dat.copy()\n",
    "        enc_dat=pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key]=0 # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "        enc_dat=pd.DataFrame(data=imp.fit_transform(enc_dat),columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix]=enc_dat.copy()\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix]=X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    min_pref = 1\n",
    "    max_pref = max_ev\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    N = len(dev_df)\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for label in labels:\n",
    "        positives[label] = sum(dev_df[label] == 1)\n",
    "        negatives[label] = sum(dev_df[label] == 0)\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in labels:\n",
    "        pir[label] = max(positives[label], negatives[label]) / positives[label]\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = mean(pir.values()) ** (1 / (4 * math.e)) + np.log(pirlbl[label])\n",
    "        negative_weights[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev'+d)]=0\n",
    "        metrics[d]['LenTrain_beforeUS_1']=sum(dev_df[d][i] for i in x_train_idx)\n",
    "        metrics[d]['LenTrain_beforeUS_0']=len(x_train_idx)-sum(dev_df[d][i] for i in x_train_idx)\n",
    "\n",
    "    if u_sample:\n",
    "        imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "        imb_ref_enc_dat['ind'] = 0\n",
    "        for i in range(len(imb_ref_enc_dat)):\n",
    "            imb_ref_enc_dat['ind'][i] = i\n",
    "\n",
    "        imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "        for trace in range(len(log)):\n",
    "            if dev_df.loc[trace].sum() > 0:\n",
    "                imb_traces['Dev'][trace] = 1\n",
    "        imb_traces = imb_traces.drop(x_test_idx)\n",
    "        imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "        X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "        x_train_idx = list(X_resampled['ind'])\n",
    "        y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "    devq_df=dev_df.loc[x_train_idx]\n",
    "    print('index length ', len(x_train_idx),len(x_test_idx),len(y_train_idx),len(y_test_idx))\n",
    "    for d in dev:\n",
    "        metrics[d]['LenTrain']=len(x_train_idx)\n",
    "        metrics[d]['LenTrain_afterUS_1']=devq_df[d].sum()\n",
    "        metrics[d]['LenTrain_afterUS_0']=len(x_train_idx)-devq_df[d].sum()\n",
    "    # validation set for early stopping\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "    enumerated_trace_idx={}\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx) # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te=X_cum[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_tr=X_cum[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_va=X_cum[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_te = y_cum_test[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va = y_cum_test[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_tr = y_cum_test[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        enumerated_trace_idx[prefix]=list(set(y_test_idx)-set(drop_idx))\n",
    "        print('subset length ',prefix, len(x_te),len(x_tr),len(y_te),len(y_tr))\n",
    "\n",
    "        if prefix ==1:\n",
    "            X_train = x_tr\n",
    "            X_test = x_te\n",
    "            y_train = y_tr\n",
    "            y_test = y_te\n",
    "            X_val = x_va\n",
    "            y_val = y_va\n",
    "        else:\n",
    "            X_train = np.append( X_train, x_tr, axis=0)\n",
    "            X_test = np.append( X_test, x_te, axis=0)\n",
    "            y_train = np.append( y_train, y_tr, axis=0)\n",
    "            y_test = np.append( y_test, y_te, axis=0)\n",
    "            y_val = np.append( y_val, y_va, axis=0)\n",
    "            X_val = np.append( X_val, x_va, axis=0)# combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train),len(y_train),len(X_val),len(y_val),len(X_test),len(y_test))\n",
    "\n",
    "    print('split done')\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BinaryClassification(no_columns=len(ref_enc_dat.loc[0]), no_devs=len(dev))\n",
    "    model.to(device)\n",
    "    criterion = nn.MultiLabelSoftMarginLoss(weight=torch.FloatTensor(list(positive_weights.values())))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if not early_stop:\n",
    "\n",
    "        print('training start no ES')\n",
    "        model.train()\n",
    "        train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                               torch.FloatTensor(y_train))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        for e in range(1, EPOCHS + 1):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = model(X_batch)\n",
    "\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                acc = binary_acc(y_pred, y_batch) / len(y_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "            print(\n",
    "                f'Epoch {e + 0:03}: | Loss: {epoch_loss / len(train_loader):.5f} | Acc: {epoch_acc / len(train_loader):.3f} |',\n",
    "                prefix)\n",
    "\n",
    "    else:\n",
    "        print('training start with ES')\n",
    "        model.train()\n",
    "        train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                               torch.FloatTensor(y_train))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "        es = EarlyStopping()\n",
    "        done = False\n",
    "\n",
    "        epoch = 0\n",
    "        while epoch < EPOCHS and not done:\n",
    "            epoch += 1\n",
    "            steps = list(enumerate(train_loader))\n",
    "            pbar = tqdm.tqdm(steps)\n",
    "            model.train()\n",
    "            epoch_acc = 0\n",
    "            epoch_loss = 0\n",
    "            for i, (x_batch, y_batch) in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                #print(y_batch_pred.shape)\n",
    "\n",
    "                loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_acc += acc.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                current = (i + 1) * len(x_batch)\n",
    "                if i == len(steps) - 1:\n",
    "                    model.eval()\n",
    "                    pred = model(X_val)\n",
    "                    vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                    if es(model, vloss): done = True\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                else:\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_data = TestData(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_pred_tag = torch.round(torch.sigmoid_(y_test_pred))\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:,i], np.array(y_pred_list)[:,i], average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_Classifier')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_BPDP_single_classifier_testcounts' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_collective_CIBE(log, ref_log, aligned_traces, u_sample=True, split=1/3, early_stop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
