{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers ----------\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------- load dataframes or pickled datasets ----------\n",
    "train_df = load_pickle(\"data_pickles/train_prefix_dataset.pkl\")\n",
    "test_df  = load_pickle(\"data_pickles/test_prefix_dataset.pkl\")\n",
    "\n",
    "# if you already built PrefixDataset (class from earlier), instantiate it:\n",
    "train_dataset = PrefixDataset(train_df, activity_col, resource_col, month_col, trace_cols, y_cols)\n",
    "test_dataset  = PrefixDataset(test_df, activity_col, resource_col, month_col, trace_cols, y_cols)\n",
    "\n",
    "# DataLoader: pin_memory True -> faster host->GPU transfer\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=128, shuffle=False,\n",
    "                          num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- model ----------\n",
    "model = LSTMCollectiveIDP(\n",
    "    activity_vocab_size=act_vocab,\n",
    "    resource_vocab_size=res_vocab,\n",
    "    month_vocab_size=month_vocab,\n",
    "    num_trace_features=len(trace_cols),\n",
    "    embedding_dim=16, lstm_hidden=128, fc_hidden=128,\n",
    "    num_output_labels=len(y_cols), dropout=0.1\n",
    ")\n",
    "model.to(device)   # move model params to GPU\n",
    "\n",
    "# optimizer + loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()  # recommended for multilabel logits\n",
    "# if using BCEWithLogitsLoss, remove final sigmoid in the model or use raw logits\n",
    "\n",
    "# ---------- optional: mixed precision ----------\n",
    "use_amp = True if device.type == \"cuda\" else False\n",
    "scaler = torch.cuda.amp.GradScaler() if use_amp else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ceadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- training loop ----------\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x_act, x_res, x_month, x_trace, y in train_loader:\n",
    "        # move batch to device efficiently\n",
    "        x_act = x_act.to(device, non_blocking=True)      # LongTensor (B, T)\n",
    "        x_res = x_res.to(device, non_blocking=True)\n",
    "        x_month = x_month.to(device, non_blocking=True)\n",
    "        x_trace = x_trace.to(device, non_blocking=True)  # FloatTensor (B, num_trace)\n",
    "        y = y.to(device, non_blocking=True)              # FloatTensor (B, num_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # forward: adapt if your model expects grouped inputs\n",
    "                logits = model(x_act, x_res, x_month, x_trace)  # prefer raw logits\n",
    "                loss = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(x_act, x_res, x_month, x_trace)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x_act.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} â€” train loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ---------- validation (optional) ----------\n",
    "    model.eval()\n",
    "    # compute metrics on test_loader...\n",
    "\n",
    "    # ---------- save checkpoint ----------\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch+1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict() if scaler is not None else None\n",
    "    }\n",
    "    torch.save(ckpt, f\"checkpoints/ckpt_epoch{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa616e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
