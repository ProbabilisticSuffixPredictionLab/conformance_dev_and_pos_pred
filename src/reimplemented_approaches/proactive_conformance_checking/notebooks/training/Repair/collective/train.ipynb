{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from joblib import load\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(0, '../..')\n",
    "sys.path.insert(0, '../../..')\n",
    "sys.path.insert(0, '../../../..')\n",
    "sys.path.insert(0, '../../../../..')\n",
    "sys.path.insert(0, '../../../../../..')\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from reimplemented_approaches.proactive_conformance_checking.models import LSTMCollectiveIDP\n",
    "from reimplemented_approaches.proactive_conformance_checking.training import Training\n",
    "from reimplemented_approaches.proactive_conformance_checking.data_prep_split_encode import PrefixDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoders:\n",
    "# Load prepared and encoded datasets\n",
    "train_set, val_set, test_set = PrefixDataset.load_datasets(save_path=\"../../../data_preparation/Repair/collective/\")\n",
    "\n",
    "print(train_set.tensors[0].size())\n",
    "print(train_set.tensors[1].size())\n",
    "print(train_set.tensors[2].size())\n",
    "print(train_set.tensors[3].size())\n",
    "print(train_set.tensors[4].size())\n",
    "\n",
    "encoders = load(\"../../../data_preparation/Repair/collective/encoders.pkl\")\n",
    "print(encoders)\n",
    "\n",
    "activity_ids = encoders.get('activity_ids') \n",
    "activity_ids_vocab_size_with_default = len(list(activity_ids.keys())) + 1 \n",
    "print(\"Activities: \", activity_ids_vocab_size_with_default)\n",
    "\n",
    "resource_ids = encoders.get('resource_ids')\n",
    "resource_ids_vocab_size_with_default = len(list(resource_ids.keys())) + 1\n",
    "print(\"Resources: \", resource_ids_vocab_size_with_default)\n",
    "\n",
    "month_ids = encoders.get('month_ids') \n",
    "month_ids_vocab_size_with_default = len(list(month_ids.keys())) + 1\n",
    "\n",
    "print(\"Months: \", month_ids_vocab_size_with_default)\n",
    "\n",
    "number_trace_attr = train_set.tensors[3].size(1)\n",
    "print(\"Number trace attributes: \", number_trace_attr)\n",
    "\n",
    "number_deviations_y = len([d for d in encoders.get('deviations')])\n",
    "print(\"Number deviatons y: \", number_deviations_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "embedding_dim = 16\n",
    "# lstm hidden size\n",
    "lstm_hidden = 128\n",
    "# fully connected hidden\n",
    "fc_hidden = 128\n",
    "# dropout probability\n",
    "p_dropout = 0.1\n",
    "\n",
    "model = LSTMCollectiveIDP(activity_vocab_size=activity_ids_vocab_size_with_default,\n",
    "                          resource_vocab_size=resource_ids_vocab_size_with_default,\n",
    "                          month_vocab_size=month_ids_vocab_size_with_default,\n",
    "                          num_trace_features=number_trace_attr,\n",
    "                          num_output_labels=number_deviations_y,\n",
    "                          # hyperparams provided in paper:\n",
    "                          embedding_dim=embedding_dim,\n",
    "                          lstm_hidden=lstm_hidden,\n",
    "                          fc_hidden=fc_hidden,\n",
    "                          dropout=p_dropout,\n",
    "                          device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not mentioned in the paper:\n",
    "# from code in paper\n",
    "batch_size=128\n",
    "# from code in paper\n",
    "shuffle = True\n",
    "epochs = 300 # 300 if early stopping (20% val from all train)\n",
    "# from code in paper\n",
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer_values = {\"optimizer\":optimizer,\n",
    "                    \"epochs\":epochs,\n",
    "                    \"mini_batches\":batch_size,\n",
    "                    \"shuffle\": shuffle}\n",
    "\n",
    "# Training with ealy stopping according to journal paper: patience:10, min_delta = 0\n",
    "training = Training(model=model,\n",
    "                    train_set=train_set,\n",
    "                    val_set=val_set,\n",
    "                    optimizer_values=optimizer_values,\n",
    "                    loss_mode = 'collective',\n",
    "                    device=device,\n",
    "                    saving_path='./LSTM_collecctive_IDP.pkl')\n",
    "\n",
    "history = training.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk_controlled_proactive_conformance_chec-7RbHgjHV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
